{"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":2977194,"sourceType":"datasetVersion","datasetId":1825054},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":6921012,"sourceType":"datasetVersion","datasetId":3972872},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7409832,"sourceType":"datasetVersion","datasetId":4309752},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":11384,"sourceType":"modelInstanceVersion","modelInstanceId":6216}],"dockerImageVersionId":30581,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":null,"end_time":null,"environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-03T18:30:37.192149","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is folked from the notebook by @YUICHI TATENO, @MARK WIJKHUIZEN and @ImperfectKitto\n\n[[infer]LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/infer-llm-detect-ai-comp-mistral-7b)  \n\n[DAIGT Mistral-7B TPU BFloat16 [Train]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)  \n\n[DAIGT Mistral-7B TPU BFloat16 [Inference]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-inference)  \n\n[LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)  \n\n\n<!-- This notebook investigates the use of an pretrained LLM to identify texts generated by another LLM.\n- The Mistral-7b-v0 was employed as an initial approach and includes cross validation and hyper-parameter tuning.\n- Fine-tuning the LLM on TPUs reduces training time from several hours (>6 hours) on GPUs to just 43 minutes. Notebook internet access must enable for TPU training.\n- Remove the typo correction task. Fine-tune LLM on larger dataset to improve the accuracy. -->","metadata":{"papermill":{"duration":0.004762,"end_time":"2023-12-03T18:30:39.134355","exception":false,"start_time":"2023-12-03T18:30:39.129593","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Install library","metadata":{"papermill":{"duration":0.00411,"end_time":"2023-12-03T18:30:39.142573","exception":false,"start_time":"2023-12-03T18:30:39.138463","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# # Install package for inferences\n# !pip install -qq --no-deps /mnt/beegfs/xchen87/ai-gen/data/daigt-pip/peft-0.6.0-py3-none-any.whl\n# !pip install -qq --no-deps /mnt/beegfs/xchen87/ai-gen/data/daigt-pip/transformers-4.35.0-py3-none-any.whl\n# !pip install -qq --no-deps /mnt/beegfs/xchen87/ai-gen/data/daigt-pip/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# !pip install -qq --no-deps /mnt/beegfs/xchen87/ai-gen/data/daigt-pip/optimum-1.14.0-py3-none-any.whl\n!pip install -qq git+https://github.com/huggingface/transformers.git  -U \n!pip install -qq git+https://github.com/huggingface/accelerate.git  -U \n!pip install -qq git+https://github.com/huggingface/optimum.git  -U \n!pip install -qq bitsandbytes \n!pip install -qq git+https://github.com/huggingface/peft.git  -U \n\n# !pip install -q /mnt/beegfs/xchen87/ai-gen/data/autocorrect/autocorrect-2.6.1.tar\n# # Eanble 4-bit CUDA functions for PyTorch\n# #!pip install -q bitsandbytes --no-index --find-link /mnt/beegfs/xchen87/ai-gen/data/llm-detect-pip/bitsandbytes-0.41.1-py3-none-any.whl\n# #!pip install -q accelerate --no-index --find-link /mnt/beegfs/xchen87/ai-gen/data/llm-detect-pip/accelerate-0.24.1-py3-none-any.whl ","metadata":{"_kg_hide-output":true,"papermill":{"duration":4.603465,"end_time":"2023-12-03T18:30:43.768151","exception":false,"start_time":"2023-12-03T18:30:39.164686","status":"completed"},"scrolled":true,"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-25T20:40:34.15854Z","iopub.execute_input":"2024-02-25T20:40:34.158845Z","iopub.status.idle":"2024-02-25T20:41:52.229553Z","shell.execute_reply.started":"2024-02-25T20:40:34.158816Z","shell.execute_reply":"2024-02-25T20:41:52.228427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install packaages for training on TPUs (notebook internet must enable)\n!pip install -qq sentencepiece==0.1.99 \n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q # Updating torch since we need the latest version\n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n# !cp /mnt/beegfs/xchen87/ai-gen/data/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"papermill":{"duration":36.090937,"end_time":"2023-12-03T18:31:19.863769","exception":false,"start_time":"2023-12-03T18:30:43.772832","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:41:52.231176Z","iopub.execute_input":"2024-02-25T20:41:52.23146Z","iopub.status.idle":"2024-02-25T20:42:26.758225Z","shell.execute_reply.started":"2024-02-25T20:41:52.231428Z","shell.execute_reply":"2024-02-25T20:42:26.757137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports for training on TPUs","metadata":{"papermill":{"duration":0.004049,"end_time":"2023-12-03T18:31:52.229976","exception":false,"start_time":"2023-12-03T18:31:52.225927","status":"completed"},"tags":[]}},{"cell_type":"code","source":"## Imports for TPU XLA \nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\nxr.use_spmd() # To enable PyTorch/XLA SPMD execution mode for automatic parallelization\n\n# \"experimental\" XLA packages\nimport torch_xla.experimental.xla_sharding as xs \nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\n# from spmd_util import partition_module","metadata":{"papermill":{"duration":0.031256,"end_time":"2023-12-03T18:31:52.265318","exception":false,"start_time":"2023-12-03T18:31:52.234062","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:42:26.759527Z","iopub.execute_input":"2024-02-25T20:42:26.759785Z","iopub.status.idle":"2024-02-25T20:42:52.694061Z","shell.execute_reply.started":"2024-02-25T20:42:26.759753Z","shell.execute_reply":"2024-02-25T20:42:52.693377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports for inference and training","metadata":{"papermill":{"duration":0.004079,"end_time":"2023-12-03T18:31:19.872111","exception":false,"start_time":"2023-12-03T18:31:19.868032","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch, transformers, sklearn, os, gc, re, random, time, sys\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom transformers import (\n    AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig,\n    LlamaConfig, GemmaConfig, AutoModel, AutoModelForCausalLM,\n    DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n) \n\nfrom accelerate import cpu_offload, dispatch_model\nfrom accelerate.utils.modeling import infer_auto_device_map\nfrom optimum.bettertransformer import BetterTransformer\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\npd.options.display.max_rows = 999\npd.options.display.max_colwidth = 99\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"papermill":{"duration":32.345259,"end_time":"2023-12-03T18:31:52.221452","exception":false,"start_time":"2023-12-03T18:31:19.876193","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:42:52.695718Z","iopub.execute_input":"2024-02-25T20:42:52.696044Z","iopub.status.idle":"2024-02-25T20:43:00.211241Z","shell.execute_reply.started":"2024-02-25T20:42:52.696016Z","shell.execute_reply":"2024-02-25T20:43:00.210138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common functions","metadata":{"papermill":{"duration":0.004011,"end_time":"2023-12-03T18:31:52.273446","exception":false,"start_time":"2023-12-03T18:31:52.269435","status":"completed"},"tags":[]}},{"cell_type":"code","source":"N_FOLD = 5\nSEED = 42\nDEBUG = False  # True: trained LLM on small samples for debugging","metadata":{"papermill":{"duration":0.01418,"end_time":"2023-12-03T18:30:39.160739","exception":false,"start_time":"2023-12-03T18:30:39.146559","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:43:00.212609Z","iopub.execute_input":"2024-02-25T20:43:00.213481Z","iopub.status.idle":"2024-02-25T20:43:00.217586Z","shell.execute_reply.started":"2024-02-25T20:43:00.213433Z","shell.execute_reply":"2024-02-25T20:43:00.216626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seed the same seed to all \ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\nseed_everything(SEED)","metadata":{"papermill":{"duration":0.013206,"end_time":"2023-12-03T18:31:52.291064","exception":false,"start_time":"2023-12-03T18:31:52.277858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:43:00.218844Z","iopub.execute_input":"2024-02-25T20:43:00.219705Z","iopub.status.idle":"2024-02-25T20:43:00.23321Z","shell.execute_reply.started":"2024-02-25T20:43:00.219646Z","shell.execute_reply":"2024-02-25T20:43:00.232375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load training data","metadata":{"papermill":{"duration":0.004119,"end_time":"2023-12-03T18:31:52.299396","exception":false,"start_time":"2023-12-03T18:31:52.295277","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Cross validation\ndef cv_split(train_data):\n    skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=42)\n    X = train_data.loc[:, train_data.columns != \"label\"]\n    y = train_data.loc[:, train_data.columns == \"label\"]\n    # Split the train into 5 folds\n    for fold, (train_index, valid_index) in enumerate(skf.split(X, X['source'])):\n        train_data.loc[valid_index, \"fold\"] = fold\n\n    print(train_data.groupby(\"fold\")[\"label\"].value_counts())\n    return train_data\n\ndef load_train_data():\n    train_df = pd.read_csv(\"/mnt/beegfs/xchen87/ai-gen/data/llm-detect-ai-generated-text/train_essays.csv\", sep=',')\n    train_prompts_df = pd.read_csv(\"/mnt/beegfs/xchen87/ai-gen/data/llm-detect-ai-generated-text/train_prompts.csv\", sep=',')\n\n    # rename column generated to label and remove used 'id' and 'prompt_id' columns\n    # Label: 1 indicates generated texts (by LLMs) \n    train_df['source'] = \"train_essay\"\n    train_df = train_df.rename(columns={'generated': 'label'})\n    train_df = train_df.reset_index(drop=True)\n    train_df = train_df.drop(['id', 'prompt_id'], axis=1)\n    \n    # Include external data\n    external_df = pd.read_csv(\"/mnt/beegfs/xchen87/ai-gen/data/daigt-v4-train-dataset/train_v4_drcat_01.csv\", sep=',')\n    # RDizzl3_seven = True\n    excluded_prompt_name_list = ['Distance learning','Grades for extracurricular activities','Summer projects']\n    external_df = external_df[~(external_df['prompt_name'].isin(excluded_prompt_name_list))]\n    # We only need 'text' and 'label' columns\n    external_df = external_df[[\"text\", \"label\", \"source\"]]\n    \n    # Merge train and external data into train_data\n    train_data = pd.concat([train_df, external_df])\n    train_data.reset_index(inplace=True, drop=True)\n    \n    # print(f\"Train data has shape: {train_data.shape}\")\n    print(f\"Train data {train_data.value_counts('label')}\") # 1: generated texts 0: human texts\n    return train_data","metadata":{"papermill":{"duration":0.014639,"end_time":"2023-12-03T18:31:52.318056","exception":false,"start_time":"2023-12-03T18:31:52.303417","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:43:00.234257Z","iopub.execute_input":"2024-02-25T20:43:00.234678Z","iopub.status.idle":"2024-02-25T20:43:00.245163Z","shell.execute_reply.started":"2024-02-25T20:43:00.234634Z","shell.execute_reply":"2024-02-25T20:43:00.244263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = load_train_data()\ntrain_data = cv_split(train_data)\n\n# Unicode decode 'NFKD'\ntrain_data['text'] = train_data['text'].str.normalize('NFKD')\ntrain_data['text'] = train_data['text'].str.encode('ascii', errors='ignore').str.decode('utf-8')\n\n# remove trailling white spaces, and ;-$ []()\ntrain_data['text'] = train_data['text'].str.strip()\ntrain_data['text'] = train_data['text'].str.replace(r'[ \\t]+$', \"\", regex=True)\ntrain_data['text'] = train_data['text'].str.replace(r'[\\;\\-\\$]', \"\", regex=True)\ntrain_data['text'] = train_data['text'].str.replace(r'[\\[\\]\\(\\)]', \"\", regex=True)\n\nAUX_LABEL_MAP = dict(zip(train_data['source'].unique(),range(train_data['source'].nunique())))\ntrain_data['aux_label'] = train_data['source'].map(AUX_LABEL_MAP)\n\ndisplay(train_data.head())","metadata":{"papermill":{"duration":2.369482,"end_time":"2023-12-03T18:31:54.69201","exception":false,"start_time":"2023-12-03T18:31:52.322528","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:43:00.246285Z","iopub.execute_input":"2024-02-25T20:43:00.246652Z","iopub.status.idle":"2024-02-25T20:43:04.746661Z","shell.execute_reply.started":"2024-02-25T20:43:00.246611Z","shell.execute_reply":"2024-02-25T20:43:04.745827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Obfuscations","metadata":{}},{"cell_type":"code","source":"# from autocorrect import Speller\n\n# speller = Speller(lang='en', fast=True)\n# train_data[\"text\"] = train_data[\"text\"].progress_apply(speller)\n\n# display(train_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-02-25T20:43:04.747772Z","iopub.execute_input":"2024-02-25T20:43:04.748344Z","iopub.status.idle":"2024-02-25T20:43:04.751508Z","shell.execute_reply.started":"2024-02-25T20:43:04.748298Z","shell.execute_reply":"2024-02-25T20:43:04.750771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # https://www3.nd.edu/~busiforc/handouts/cryptography/Letter%20Frequencies.html\n# BIGRAMS = ['th', 'he', 'in', 'en', 'nt', 're', 'er', 'an', 'ti', 'es', 'on', 'at', 'se',\n#            'nd', 'or', 'ar', 'al', 'te', 'co', 'de', 'to', 'ra', 'et', 'ed']\n# TRIGRAMS = ['the', 'and', 'tha', 'ent', 'ing', 'ion', 'tio', 'for','nde', 'has', 'nce']\n\n# def replace_uni(text):\n#     bi = random.choice(BIGRAMS[:10])\n#     c1,c2 = random.sample(bi, 2)\n#     return text.replace(c1, c2)\n\n# def replace_bi(text):\n#     bi = random.choice(BIGRAMS)\n#     c = random.choice(bi)\n#     return text.replace(c, bi)\n\n# def replace_tri(text):\n#     ti = random.choice(TRIGRAMS)\n#     c = random.choice(ti)\n#     return text.replace(c, ti)\n    \n# def Obfuscate(text, P=0.3):\n#     if random.random() < P:\n#         n = random.random()\n#         if n<0.4:\n#             text = replace_uni(text)\n#         elif n<0.8:\n#             text = replace_bi(text)\n#         else:\n#             text = replace_tri(text)\n#     return text","metadata":{"execution":{"iopub.status.busy":"2024-02-25T20:43:04.753922Z","iopub.execute_input":"2024-02-25T20:43:04.754197Z","iopub.status.idle":"2024-02-25T20:43:04.76201Z","shell.execute_reply.started":"2024-02-25T20:43:04.754169Z","shell.execute_reply":"2024-02-25T20:43:04.761361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data.loc[train_data['source']!='train_essay', 'text'] = train_data.loc[train_data['source']!='train_essay', 'text'].progress_apply(Obfuscate)\ntrain_data[\"text\"] = \"Check whether the following essay is human written or AI generated? Remember, the text might have undergone multiple stages of obfuscation.\\n\" + train_data[\"text\"]\n\ndisplay(train_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-02-25T20:43:04.762746Z","iopub.execute_input":"2024-02-25T20:43:04.762972Z","iopub.status.idle":"2024-02-25T20:43:04.776274Z","shell.execute_reply.started":"2024-02-25T20:43:04.762949Z","shell.execute_reply":"2024-02-25T20:43:04.775647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the LLM model on TPUs\n\nNLTK package is used to correct ","metadata":{"papermill":{"duration":0.00496,"end_time":"2023-12-03T18:31:54.702442","exception":false,"start_time":"2023-12-03T18:31:54.697482","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor, as_completed\n# from textblob import TextBlob\n\ndef pre_processing_text(text):\n    corrected_text = text.replace('\\n', ' ')\n    #corrected_text = TextBlob(corrected_text).correct()\n    return corrected_text\n\n# def parallelize_processing_texts(texts):\n#     corrected_texts = []\n#     # Run the spelling correction in parallel with a progress bar\n# #     with tqdm(total=len(texts)) as pbar:\n#     with ThreadPoolExecutor() as executor:\n#         futures = list(executor.map(pre_processing_text, texts))\n#         for future in as_completed(futures):\n#             corrected_text = future.result()\n#             corrected_texts.append(corrected_text)\n#         if len(corrected_texts) % 100 == 0:    \n#             print(f\"Complete {len(corrected_texts)} / {len(texts)}\")\n#     return corrected_texts","metadata":{"papermill":{"duration":0.013955,"end_time":"2023-12-03T18:31:54.721267","exception":false,"start_time":"2023-12-03T18:31:54.707312","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:43:04.777118Z","iopub.execute_input":"2024-02-25T20:43:04.777378Z","iopub.status.idle":"2024-02-25T20:43:04.791839Z","shell.execute_reply.started":"2024-02-25T20:43:04.77735Z","shell.execute_reply":"2024-02-25T20:43:04.791113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# MODEL\n# ====================================================\nclass GemmaForSequenceClassification(torch.nn.Module):\n    def __init__(self, tokenizer, TARGET_MODEL, DEVICE, LORA=False, model_name='gemma2b'):\n        super().__init__()\n        self.DEVICE = DEVICE\n        base_model = AutoModel.from_pretrained(TARGET_MODEL, torch_dtype=torch.bfloat16, trust_remote_code=True)\n        # No idea why this is needed\n        base_model.config.pretraining_tp = 1 # 1 is 7b\n        # Assign Padding TOKEN\n        base_model.config.pad_token_id = tokenizer.pad_token_id\n        # Unfreeze all layers\n        if LORA:\n            # LoRa\n            peft_config = LoraConfig(\n                r=64,  # Use larger 'r' value increase more parameters during training\n                lora_alpha=16,\n                lora_dropout=0.10,\n                bias='none',\n                inference_mode=False,\n                # Only Use Output and Values Projection\n                target_modules=[\n                    'o_proj',\n                    'v_proj',\n                ],\n            )\n            # Load the new PEFT model\n            self.base_model = get_peft_model(base_model, peft_config)\n        else:\n            self.base_model = base_model\n        # Classification Dropout\n        self.head_drop = torch.nn.Dropout(p=0.15)\n        # Classification Heads\n        self.llm_head = torch.nn.Linear(base_model.config.hidden_size, 1, bias=False).to(self.DEVICE)\n        self.llm_head_aux = torch.nn.Linear(base_model.config.hidden_size, len(AUX_LABEL_MAP), bias=False).to(self.DEVICE)\n        # Print Model Prameters\n        if LORA:\n            self.base_model.print_trainable_parameters()\n        else:\n            total_params = sum(p.numel() for p in self.base_model.parameters())\n            print(f\"Total Trainable parameters: {total_params}\")\n        \n    def get_seq_lens(self, input_ids):\n        return (torch.eq(input_ids, self.base_model.config.pad_token_id).int().argmax(-1) - 1).to(self.DEVICE)\n    \n    def get_pooled_out(self, hidden_states, input_ids):\n        bs_r = torch.arange(hidden_states.shape[0], device=self.DEVICE)\n        return hidden_states[bs_r, self.get_seq_lens(input_ids)].to(self.DEVICE)\n\n    def forward(self, input_ids, attention_mask):\n        hidden_states = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        outputs = self.get_pooled_out(hidden_states[0].to(dtype=torch.float32), input_ids=input_ids)\n        outputs = self.head_drop(outputs)\n        return self.llm_head(outputs), self.llm_head_aux(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T20:43:04.792727Z","iopub.execute_input":"2024-02-25T20:43:04.792972Z","iopub.status.idle":"2024-02-25T20:43:04.803534Z","shell.execute_reply.started":"2024-02-25T20:43:04.792945Z","shell.execute_reply":"2024-02-25T20:43:04.802835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Partition Stratagy","metadata":{}},{"cell_type":"code","source":"# From this repo: https://github.com/HeegyuKim/torch-xla-SPMD\nGEMMA_RULES = (\n    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n    (\"lm_head\", (\"fsdp\", \"mp\")),\n)\n\nALL_RULES = [\n#     (GPTNeoXConfig, GPTNEOX_RULES),\n#     (T5Config, T5_RULES),\n#     (LlamaConfig, LLAMA_RULES),\n#     (MixtralConfig, MIXTRAL_RULES),\n    (GemmaConfig, GEMMA_RULES)\n]\ndef find_rule(model):\n    for config, rule in ALL_RULES:\n        if model.config.__class__ == config:\n            return rule\n    raise Exception(\"unsupported model to partitioning\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T20:43:04.804421Z","iopub.execute_input":"2024-02-25T20:43:04.804667Z","iopub.status.idle":"2024-02-25T20:43:04.816503Z","shell.execute_reply.started":"2024-02-25T20:43:04.804641Z","shell.execute_reply":"2024-02-25T20:43:04.815754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strkey2id = {\n    \"dp\": 0,\n    \"fsdp\": 1,\n    \"mp\": 2\n}\n\ndef partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n    partition_specs = find_rule(model)\n    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n    # print(rule)\n\n    for name, module in model.named_modules():\n        module.to(device)\n        # print(name, module.__class__.__name__)\n        if isinstance(module, (nn.Embedding, nn.Linear)):\n            for rule_pattern, spec in rule:\n                if re.findall(rule_pattern, name):\n                    if verbose:\n                        print(\"match\", rule_pattern, name)\n                    \n                    xs.mark_sharding(module.weight, mesh, spec)\n                    break","metadata":{"execution":{"iopub.status.busy":"2024-02-25T20:43:04.81737Z","iopub.execute_input":"2024-02-25T20:43:04.817618Z","iopub.status.idle":"2024-02-25T20:43:04.832973Z","shell.execute_reply.started":"2024-02-25T20:43:04.817592Z","shell.execute_reply":"2024-02-25T20:43:04.832291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\n\nclass TrainModelTPU():\n    def __init__(self, fold, epochs, train_data):\n        self.fold = fold\n        self.train_data = train_data\n        self.DEBUG = DEBUG\n        self.SEED = SEED\n        self.NUM_LABELS = 1 # Total Number of Labels (0:human texts, 1:LLM generated texts)\n        self.MAX_LENGTH = 1024\n        self.BATCH_SIZE = 8\n        self.LR = 5e-6 # Learning rate\n        self.WD = 0.1 # Weight Decay Ratio\n        self.AUX_WEIGHT = 0.4 # Auxilary Loss Weight\n        self.DEVICE = xm.xla_device() # Initialize TPU Device\n        self.NUM_EPOCHS = epochs # Number Of Training Epochs\n        self.NUM_WARMUP_STEPS = 60 # Number of Warmup Steps\n        # Load the baseline model\n        self.TARGET_MODEL = \"/mnt/beegfs/xchen87/ai-gen/data/gemma/transformers/2b/2\"\n        self.LORA = False # Unfreeze all layers\n        # Model saving path\n        self.SAVE_PATH = f'/kaggle/working/gemma2b_fold{fold}_TPU'\n        self.load_model()  # Load the pretrained LLM and tokenizer \n    \n    # Disply trainable layers of LLM\n    def display_model_layers(self):        \n        # Dispaly trainable layers for verification\n        trainable_layers = []\n        n_trainable_params = 0\n        for name, param in self.model.named_parameters():\n            # Layer Parameter Count\n            n_params = int(torch.prod(torch.tensor(param.shape)))\n            # Only Trainable Layers\n            if param.requires_grad:\n                # Add Layer Information\n                trainable_layers.append({\n                    '#param': n_params,\n                    'name': name,\n                    'dtype': param.data.dtype,\n                    'params': param\n                })\n                n_trainable_params += n_params\n\n        display(pd.DataFrame(trainable_layers))\n        print(f\"Number of trainable parameters: {n_trainable_params:,} \"\n              f\"Number of trainable layers: {len(trainable_layers)}\")\n        \n    def unfreeze_last_nlayers(self, model, n=1):\n        num_layers = model.config.num_hidden_layers\n        unfreeze_layers = [f'layers.{i}' for i in range(num_layers-n, num_layers)]\n        for name, param in model.named_parameters():\n            for u in unfreeze_layers:\n                if u in name:\n                    param.requires_grad = True\n        \n    # Load pretrained LLM and tokenizer\n    def load_model(self):\n        # Load the tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(self.TARGET_MODEL, use_fast=False)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        self.model = GemmaForSequenceClassification(self.tokenizer, self.TARGET_MODEL, self.DEVICE, self.LORA)\n        self.unfreeze_last_nlayers(self.model.base_model, n=1)\n#         # No idea why this is needed\n#         base_model.config.pretraining_tp = 1 # 1 is 7b\n#         # Assign Padding TOKEN\n#         base_model.config.pad_token_id = self.tokenizer.pad_token_id\n\n#         # LoRa\n#         peft_config = LoraConfig(\n#             r=16 if self.DEBUG else 64,  # Use larger 'r' value increase more parameters during training\n#             lora_alpha=16,\n#             lora_dropout=0.10,\n#             bias='none',\n#             inference_mode=False,\n#             # Only Use Output and Values Projection\n#             target_modules=[\n#                 'o_proj',\n#                 'v_proj',\n#             ],\n#         )\n#         # Create LoRa Model\n# #         if self.epoch > 1:\n# #             print(f\"Continue Training the model from {str(self.TARGET_MODEL)}\")\n# #             # Continue training PEFT model\n# #             self.model = PeftModel.from_pretrained(base_model, str(self.TARGET_MODEL))\n# #         else:\n#         # Load the new PEFT model\n#         self.model = get_peft_model(base_model, peft_config)\n#         self.head_drop = torch.nn.Dropout(p=0.1)\n#         self.llm_head = torch.nn.Linear(base_model.config.hidden_size, 1, bias=False)\n#         # Display Trainable Parameters to make sure we load the model successfully\n#         self.model.print_trainable_parameters()\n# #         if self.DEBUG:\n# #             self.display_model_layers()\n        print(\"Complete loading pretrained LLM model\")\n    \n    def create_optimizer_scheduler(self, STEPS_PER_EPOCH, NUM_EPOCHS):        \n        # Optimizer (Adam)\n        if self.WD:\n            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.LR, weight_decay=self.LR*self.WD)\n        else:\n            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.LR)\n\n        # Cosine Learning Rate With Warmup\n        lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n                                    optimizer=optimizer,\n                                    num_warmup_steps=self.NUM_WARMUP_STEPS,\n                                    num_training_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\n        # Set the data type for the optimizer's state (e.g., momentum buffers)\n        for state in optimizer.state.values():\n            for k, v in state.items():\n                if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n                    state[v] = v.to(dtype=torch.float32)\n        print(\"Complete creating optimizer and lr scheduler\")\n        print(\"optimizer\", optimizer)\n        print(\"lr_scheduler\", lr_scheduler)\n        return optimizer, lr_scheduler\n        \n    def partition_mesh(self):\n        # Number of TPU Nodes to ensure we can access TPUs and partition the model into mesh\n        num_devices = xr.global_runtime_device_count()\n        mesh_shape = (1, num_devices, 1)\n        device_ids = np.array(range(num_devices))\n        mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n        partition_module(self.model.base_model, mesh)\n        return num_devices, mesh\n    \n    def tokenize(self, text):\n        # Tokenize Data\n        tokens = self.tokenizer(text, # Texts\n                                padding='max_length', # Pad texts to maximum length\n                                max_length=self.MAX_LENGTH, # Maximum token length\n                                truncation=True, # Truncate texts if they are too long\n                                return_tensors='np', # Return Numpy array\n                                )\n        # Input IDs are the token IDs\n        INPUT_IDS = tokens['input_ids'][:,:self.MAX_LENGTH]\n        # Attention Masks to Ignore Padding Tokens\n        ATTENTION_MASKS = tokens['attention_mask'][:,:self.MAX_LENGTH]\n        return INPUT_IDS, ATTENTION_MASKS\n        \n     # Create a training dataset \n    def create_dataset(self, TEXTS, GENERATED, AUX_LABELS, N_SAMPLES, mesh):\n        IDXS = np.arange(N_SAMPLES-(N_SAMPLES%self.BATCH_SIZE))\n        while True:\n            # Shuffle Indices\n            np.random.shuffle(IDXS)\n            # Iterate Over All Indices Once\n            for idxs in IDXS.reshape(-1, self.BATCH_SIZE):\n                # Tokenize\n                input_ids, attention_mask = self.tokenize(TEXTS[idxs].tolist())\n                # convert to torch tensors\n                input_ids = torch.tensor(input_ids).to(self.DEVICE)\n                attention_mask = torch.tensor(attention_mask).to(self.DEVICE)\n                labels = torch.tensor(GENERATED[idxs]).to(self.DEVICE)\n                aux_labels = torch.tensor(AUX_LABELS[idxs]).to(self.DEVICE)\n                # Shard Over TPU Nodes\n                xs.mark_sharding(input_ids, mesh, (0, 1))\n                xs.mark_sharding(attention_mask, mesh, (0, 1))\n                xs.mark_sharding(labels, mesh, (0, 1))\n                xs.mark_sharding(aux_labels, mesh, (0, 1))\n                yield input_ids, attention_mask, labels, aux_labels\n    \n    # Save the trained model as output files\n    def save_model(self):\n        self.model = self.model.cpu()# Move model first on CPU before saving weights \n        self.model.base_model.save_pretrained(self.SAVE_PATH) # Save the entire fine-tuned model\n        self.tokenizer.save_pretrained(self.SAVE_PATH) # Save tokenizer for inference\n        # Save the LLM classification head\n        torch.save(self.model.llm_head.cpu().state_dict(), f\"{self.SAVE_PATH}/llm_head.pth\")     \n        # Save all the trainable parameters\n        torch.save(dict([(k,v) for k, v in self.model.named_parameters() if v.requires_grad]), f\"{self.SAVE_PATH}/model.pth\")\n        print(f\"Save the model and tokenizers to {self.SAVE_PATH}\")\n    \n      # Validate the model\n    def valid_model(self):\n        num_devices, mesh = self.partition_mesh()\n        # print(f'Number_DEVICES: {num_devices}')\n        \n        # Use the non-fold dataset as valid dataset\n        fold_valid_df = self.train_data[self.train_data[\"fold\"] == self.fold]\n        if self.DEBUG:\n            fold_valid_df = fold_valid_df.sample(n=100, random_state=self.SEED)  # Validate on small samples \n        \n        # Compute total samples and number of steps in one epochs\n        N_SAMPLES = len(fold_valid_df)\n        print(f\"Start validating the model with number of sample {N_SAMPLES}\")\n\n        # Generated By AI Label of Texts\n        TEXTS = fold_valid_df['text'].values\n        GENERATED = fold_valid_df['label'].values.reshape(-1,1).astype(np.float32)\n        AUX_LABELS = fold_valid_df['aux_label'].values.reshape(-1,1).astype(np.float32)\n        # Create a valid dataset \n        VALID_DATASET = self.create_dataset(TEXTS, GENERATED, AUX_LABELS, N_SAMPLES, mesh)\n\n        # Put Model In Eval Modus\n        self.model.eval()\n        # Loss Function\n        LOSS_FN = torch.nn.BCEWithLogitsLoss().to(dtype=torch.float32)\n        METRICS = {'loss': [], \n                   'auc': {'y_true': [], 'y_pred': []} }\n        STEPS = N_SAMPLES // self.BATCH_SIZE\n        for step in tqdm(range(STEPS)):\n            # Enable inference mode using `no_grad`\n            with torch.no_grad():\n                start = time.time()\n                # Get Batch\n                input_ids, attention_mask, labels, _ = next(VALID_DATASET)\n                 # Forward Pass\n                outputs, _ = self.model(input_ids=input_ids, attention_mask=attention_mask)\n                # Logits Float32\n                logits = outputs.to(dtype=torch.float32)\n                # Backward Pass\n                loss = LOSS_FN(logits, labels)\n                # Update Metrics And Progress Bar\n                METRICS['loss'].append(float(loss))\n                METRICS['auc']['y_true'] += labels.squeeze().tolist()\n                METRICS['auc']['y_pred'] += logits.sigmoid().tolist()\n\n        # Compute and display the validation results\n        print(f\"Number of validation data {len(fold_valid_df)}\\n\"\n              f\"loss (valid): {np.mean(METRICS['loss']): .3f}\\n\"\n              f\"auc (valid): {sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'], METRICS['auc']['y_pred']):.3f}\")\n \n    # Train the model by the fold data\n    def train_model(self):\n        num_devices, mesh = self.partition_mesh()\n        print(f'Number_DEVICES: {num_devices}')\n        # Use the fold data as training data\n        fold_train_df = self.train_data[self.train_data[\"fold\"] != self.fold]\n        if self.DEBUG:\n            fold_train_df = fold_train_df.sample(frac =.1, random_state=SEED) # Select a small amount of samples \n        # limited to two columns\n        fold_train_df = fold_train_df[['text', 'label', 'aux_label']]\n#         print(\"Start correcting the typos in training dataset\")\n#         start = time.time()\n#         # Preprocess the text to correct the typos \n#         fold_train_df['text'] = fold_train_df['text'].map(lambda text: pre_processing_text(text))\n#         print(f\"Complete correcting the texts {len(fold_train_df['text'])} in {time.time() - start : .1f}\")\n        # Compute total samples and number of steps in one epochs\n        N_SAMPLES = len(fold_train_df)\n        # Compute the total steps per epochs\n        STEPS_PER_EPOCH = N_SAMPLES // self.BATCH_SIZE\n        print(f'BATCH_SIZE: {self.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')\n        \n        # Generated By AI Label of Texts\n        TEXTS = fold_train_df['text'].values\n        GENERATED = fold_train_df['label'].values.reshape(-1,1).astype(np.float32)\n        AUX_LABELS = fold_train_df['aux_label'].values.reshape(-1,1).astype(np.float32)\n        print(f'TEXTS shape: {TEXTS.shape}\\n'\n              f'AUX_LABELS shape: {AUX_LABELS.shape}\\n'\n              f'GENERATED shape: {GENERATED.shape}')\n        \n        # Create a train dataset\n        TRAIN_DATASET = self.create_dataset(TEXTS, GENERATED, AUX_LABELS, N_SAMPLES, mesh)\n        \n        # Create optimizer and lr_scheduler\n        optimizer, lr_scheduler = self.create_optimizer_scheduler(STEPS_PER_EPOCH, self.NUM_EPOCHS)\n        \n        # Put Model In Train Modus\n        self.model.train()\n        # Loss Function, basic Binary Cross Entropy\n        LOSS_FN = torch.nn.BCEWithLogitsLoss().to(dtype=torch.float32)\n        LOSS_FN_AUX = torch.nn.CrossEntropyLoss(label_smoothing=0.1).to(dtype=torch.float32)\n        # Training loop goes through each epoch\n        for epoch in tqdm(range(self.NUM_EPOCHS)):\n            METRICS = {'loss': [], \n                       'auc': {'y_true': [], 'y_pred': []} }\n            # Go through each step\n            for step in range(STEPS_PER_EPOCH):\n                # Zero Out Gradients\n                optimizer.zero_grad()\n                # Get Batch\n                input_ids, attention_mask, labels, aux_labels = next(TRAIN_DATASET)\n                # Test the TRAIN_DATASET for debugging first record\n                # Forward Pass\n                outputs, aux_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n                # Logits Float32\n                logits = outputs.to(dtype=torch.float32)\n                aux_logits = aux_outputs.to(dtype=torch.float32)\n                # Backward Pass\n                if self.AUX_WEIGHT:\n                    loss = LOSS_FN(logits, labels)\n                    aux_loss = LOSS_FN_AUX(aux_logits, aux_labels.squeeze())\n                    loss = (loss * (1-self.AUX_WEIGHT)) + (aux_loss * self.AUX_WEIGHT)\n                    loss.backward()\n                else:\n                    loss = LOSS_FN(logits, labels)\n                    loss.backward()\n                # Update Weights\n                optimizer.step()\n                xm.mark_step()\n                # Update Learning Rate Scheduler\n                lr_scheduler.step()\n                # Update Metrics And Progress Bar\n                METRICS['loss'].append(float(loss))\n                METRICS['auc']['y_true'] += labels.squeeze().tolist()\n                METRICS['auc']['y_pred'] += logits.sigmoid().tolist()\n                # print(f\"Complete updating metrics {METRICS}\")\n                # Metrics Shown After Both Classes Present\n                if np.unique(METRICS['auc']['y_true']).size == 2:\n                    metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n                    metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n                    metrics += ', µ_auc: {:.3f}'.format(\n                        sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'], METRICS['auc']['y_pred'])\n                    )\n\n                    lr = optimizer.param_groups[0]['lr']\n                    print('\\r'*100, f'{epoch+1:02}/{self.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n            avg_loss = np.mean(METRICS['loss'])\n            roc_auc_score = sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'],\n                                                          METRICS['auc']['y_pred'])\n            print(f'\\nFinish EPOCH {epoch} with average_loss: {avg_loss: .5f} '\n                  f'ROC_AUC_Score: {roc_auc_score:.5f}')\n            \n    # Clear the memory\n    def clear_memory(self):\n        del self.model, self.tokenizer\n        libc.malloc_trim(0)\n        gc.collect()","metadata":{"papermill":{"duration":0.047126,"end_time":"2023-12-03T18:31:54.773293","exception":false,"start_time":"2023-12-03T18:31:54.726167","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:43:04.83411Z","iopub.execute_input":"2024-02-25T20:43:04.834388Z","iopub.status.idle":"2024-02-25T20:43:04.876083Z","shell.execute_reply.started":"2024-02-25T20:43:04.83436Z","shell.execute_reply":"2024-02-25T20:43:04.875282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"as_completed# Start training the model\ndisplay(train_data.head(3))\n\nfold = 0\nepochs = 1\ntrainer = TrainModelTPU(fold, epochs, train_data)\n\n# Train the model\ntrainer.train_model()\n# Validate the model\ntrainer.valid_model()\n\n# Save model and tokenizer\ntrainer.save_model()\n \n# Clear the memorys\ntrainer.clear_memory()\ndel trainer","metadata":{"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2023-12-03T18:31:54.778131","status":"running"},"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:43:04.877067Z","iopub.execute_input":"2024-02-25T20:43:04.877371Z","iopub.status.idle":"2024-02-25T20:47:58.297004Z","shell.execute_reply.started":"2024-02-25T20:43:04.877342Z","shell.execute_reply":"2024-02-25T20:47:58.295902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Start training the model\n# display(train_data.head(3))\n# try:\n#     fold = 0\n#     # train on fold-th data and epoch-th\n#     for epoch in list(range(1, 11)):\n#         trainer = TrainModelTPU(fold, epoch, train_data)\n#         trainer.train_model() # Train the model    \n#         trainer.valid_model() # Validate the model\n#         trainer.save_model() # Save model and tokenizer\n#         trainer.clear_memory()\n#         del trainer\n# except:\n#     print(\"Something is wrong during training\")\n#     sys.exit(-1) # Stop with error\n# print(f\"Complete training the model on fold {fold}\")\n# sys.exit(0) # Stop the notebook normally\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:47:58.29902Z","iopub.execute_input":"2024-02-25T20:47:58.299339Z","iopub.status.idle":"2024-02-25T20:47:58.303839Z","shell.execute_reply.started":"2024-02-25T20:47:58.29929Z","shell.execute_reply":"2024-02-25T20:47:58.302837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#     from __future__ import annotations\n#     import time, sys, gc, logging, random\n#     from pathlib import Path\n#     import numpy as np\n#     import pandas as pd\n#     from tqdm import tqdm\n#     from datasets import Dataset\n#     from sklearn.model_selection import StratifiedKFold\n#     from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig \n#     from peft import TaskType, prepare_model_for_kbit_training, AutoPeftModelForCausalLM # type: ignore\n#     from transformers import BitsAndBytesConfig\n#     import torch\n\n#     from transformers import logging as hf_logging\n#     from transformers import (\n#         AutoTokenizer, AutoModelForSequenceClassification, LlamaForSequenceClassification,\n#         TrainingArguments, Trainer, DataCollatorWithPadding)\n#     import transformers\n\n#     import peft\n#     from accelerate import Accelerator\n#     import bitsandbytes\n#     from sklearn.metrics import accuracy_score, roc_auc_score\n#     from shutil import rmtree\n#     import language_tool_python\n#     import optuna\n#     import concurrent\n#     from concurrent.futures import ThreadPoolExecutor\n#     from concurrent.futures import wait\n\n#     print(transformers.__version__)\n#     print(peft.__version__)\n#     print(torch.__version__)\n\n#     ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-02-25T20:47:58.304968Z","iopub.execute_input":"2024-02-25T20:47:58.305224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class TrainModelGPU:\n#     def __init__(self, fold):\n#         self.fold = fold\n#         self.TARGET_MODEL = \"/mnt/beegfs/xchen87/ai-gen/data/mistral/pytorch/7b-v0.1-hf/1\"\n#         self.load_model()\n\n#     # Load the pretrained model and add an extra layer with PEFT library for fine-tuning\n#     def load_model(self):\n#         # Enable GPU to run the model with 4bit\n#         bnb_config = BitsAndBytesConfig(\n#             load_in_4bit=True,\n#             bnb_4bit_quant_type=\"nf4\",\n#             bnb_4bit_use_double_quant=True,\n#             bnb_4bit_compute_dtype=torch.bfloat16\n#         )\n#         # LoRA: Low-Rank Adaptation of Large Language Models is a popular approach to fine-tune LLM on a single GPU\n#         # https://abvijaykumar.medium.com/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-2-d8e23877ac6f\n#         lora_config = LoraConfig(            \n#             r=16, # A larger 'r' value needs to update more parameters \n#             lora_dropout=0.10,\n#             bias='none',\n#             task_type=TaskType.SEQ_CLS,\n#             inference_mode=False,\n#             #Only targeting attention blocks of the model or targeting all linear layers\n#             target_modules=[\"q_proj\", \"v_proj\"] # if DEBUG else ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head'],                \n#         )\n#         # Load the tokenizer\n#         self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=False)\n#         self.tokenizer.pad_token = self.tokenizer.eos_token\n\n#         base_model = LlamaForSequenceClassification.from_pretrained(TARGET_MODEL,\n#                                                                     num_labels=2, # label is 0 or 1\n#                                                                     quantization_config=bnb_config,                                                                 \n#                                                                     device_map=\"auto\")\n\n#         base_model.config.pretraining_tp = 1 # 1 is 7b\n#         base_model.config.pad_token_id = tokenizer.pad_token_id\n\n        \n#         # Load the model\n#         start = time.time()    \n#         # Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of \n#         # pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. \n#         # training_model = prepare_model_for_kbit_training(base_model)\n#         model = get_peft_model(base_model, lora_config)\n#         print(f\"Complete loading the base model {time.time() - start: .1f} seconds\") \n#         model.print_trainable_parameters() # Display the trainable parameters\n        \n#         return model, tokenizer\n    \n#     def preprocess_function(self, examples, tokenizer, max_length=512):\n#         examples[\"text\"] = list(map(lambda text: self.pre_processing_text(text), examples[\"text\"]))\n#         return tokenizer(examples[\"text\"], truncation=True, max_length=max_length, padding=True)\n\n#     def compute_metrics(self, eval_pred):\n#         predictions, labels = eval_pred\n#         predictions = np.argmax(predictions, axis=1)\n\n#         accuracy_val = accuracy_score(labels, predictions)\n#         roc_auc_val = roc_auc_score(labels, predictions)\n#         r = { \"accuracy\": accuracy_val,\n#               \"roc_auc\": roc_auc_val}\n#         # logging.debug(f'{r}')\n#         return r\n\n#     # Fine Tuning LLM: Parameter Efficient Fine Tuning (PEFT) and Lora configurations\n#     # https://github.com/microsoft/LoRA\n#     def train_model_by_fold(self, fold):\n#         torch.cuda.empty_cache()\n#         gc.collect()\n#         print(f\"Start training the fold {fold} model\")\n#         # Create train and valid dataset for a fold\n#         fold_valid_df = train_data[train_data[\"fold\"] == fold]\n#         fold_train_df = train_data[train_data[\"fold\"] != fold]\n#         # Train the model with small (for debugging) or large samples\n#         if DEBUG:\n#             fold_train_df = fold_train_df.sample(frac =.1, random_state=SEED)\n#             fold_valid_df = fold_valid_df.sample(frac =.3, random_state=SEED)\n#         else:\n#             fold_train_df = fold_train_df.sample(frac =.3, random_state=SEED)\n#             fold_valid_df = fold_valid_df.sample(frac =1.0, random_state=SEED)\n\n#         print(f'fold_train_df: Len = {len(fold_train_df)} Counts = {fold_train_df.groupby(\"fold\")[\"label\"].value_counts()}')\n#         print(f'fold_valid_df: Len = {len(fold_valid_df)} Counts = {fold_valid_df.groupby(\"fold\")[\"label\"].value_counts()}')\n#         # create the dataset\n#         train_ds = Dataset.from_pandas(fold_train_df)\n#         valid_ds = Dataset.from_pandas(fold_valid_df)\n\n#         # Load the pretrained model and tokenizer\n#         model, tokenizer = load_model(fold)\n\n#         # Tokenize the train and valid dataset and pass tokenizer as function argument\n#         train_tokenized_ds = train_ds.map(self.preprocess_function, batched=True,\n#                                           fn_kwargs={\"tokenizer\": tokenizer})\n#         valid_tokenized_ds = valid_ds.map(self.preprocess_function, batched=True,\n#                                           fn_kwargs={\"tokenizer\": tokenizer})\n#         # Create data collator with padding (padding to the longest sequence)\n#         data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n\n#         # Start training processing        \n#         TMP_DIR = Path(f\"/kaggle/tmp/mistral_7b_fold{fold}/\")\n#         TMP_DIR.mkdir(exist_ok=True, parents=True)\n\n#         STEPS = 5 if DEBUG else 20\n#         EPOCHS = 1 if DEBUG else 10\n#         LEARNING_RATE = 1e-4 \n#         training_args = TrainingArguments(output_dir=TMP_DIR,\n#                                           overwrite_output_dir=True,\n#                                           fp16=True, #converts to float precision 16 using bitsandbytes\n#                                           learning_rate=LEARNING_RATE,                                      \n#                                           per_device_train_batch_size=1,\n#                                           per_device_eval_batch_size=1,\n#                                           gradient_accumulation_steps=16,\n#                                           max_grad_norm=0.3,\n#                                           optim='paged_adamw_32bit',\n#                                           lr_scheduler_type=\"cosine\",\n#                                           num_train_epochs=EPOCHS,\n#                                           weight_decay=0.01,\n#                                           evaluation_strategy=\"epoch\",\n#                                           save_strategy=\"epoch\",\n#                                           load_best_model_at_end=True,\n#                                           metric_for_best_model=\"roc_auc\",\n#                                           label_names=[\"label\"],\n#                                           push_to_hub=False,\n#                                           warmup_steps=STEPS,\n#                                           eval_steps=STEPS,\n#                                           logging_steps=STEPS,\n#                                           report_to='none', # if DEBUG else 'wandb'\n#                                           log_level='warning', # 'warning' is default level \n#                                          )\n#         start = time.time()\n#         # Create the trainer \n#         trainer = Trainer(model=model,\n#                           args=training_args,\n#                           train_dataset=train_tokenized_ds,\n#                           eval_dataset=valid_tokenized_ds,\n#                           tokenizer=tokenizer,\n#                           data_collator=data_collator,\n#                           compute_metrics=self.compute_metrics)\n\n#         trainer.train()\n\n#         OUTPUT_DIR = Path(f\"/kaggle/working/mistral_7b_fold{fold}/\")\n#         OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n#         # Save the full model and the training arguments\n#         trainer.save_model(str(OUTPUT_DIR))\n#         print(f\"=== Finish the training for fold {fold} in {time.time() - start:.1f} seconds ===\")\n#         del model, trainer, tokenizer\n#         torch.cuda.empty_cache()\n#         gc.collect()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}