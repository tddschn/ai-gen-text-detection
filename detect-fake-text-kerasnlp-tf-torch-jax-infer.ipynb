{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6300093,"sourceType":"datasetVersion","datasetId":3623988},{"sourceId":6857742,"sourceType":"datasetVersion","datasetId":3623154},{"sourceId":6987454,"sourceType":"datasetVersion","datasetId":3947266},{"sourceId":151069021,"sourceType":"kernelVersion"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLM - Detect AI Generated Text\n> Identify which essay was written by a large language model\n\n<img src=\"https://user-images.githubusercontent.com/36858976/279902422-b365f6ef-ef01-49ac-af7f-0bc2ca3ba835.png\">","metadata":{}},{"cell_type":"markdown","source":"# üéØ | Motivation\n\n* In this notebook, we will demonstrate the usage of the multi-backend capabilities of `KerasCore` and `KerasNLP` for the **Detect Fake Text** infernece.","metadata":{}},{"cell_type":"markdown","source":"# üìì | Notebooks\n\n* Train: [Detect Fake Text: KerasNLP [TF/Torch/JAX][Train]](https://www.kaggle.com/code/awsaf49/detect-fake-text-kerasnlp-tf-torch-jax-train)\n* Infer: [Detect Fake Text: KerasNLP [TF/Torch/JAX][Infer]](https://www.kaggle.com/code/awsaf49/detect-fake-text-kerasnlp-tf-torch-jax-infer)","metadata":{}},{"cell_type":"markdown","source":"# üõ† | Install Libraries ","metadata":{}},{"cell_type":"code","source":"!pip install /mnt/beegfs/xchen87/ai-gen/data/llm-science-exam-lib-ds/keras_core-0.1.7-py3-none-any.whl --no-deps\n!pip install /mnt/beegfs/xchen87/ai-gen/data/llm-science-exam-lib-ds/keras_nlp-0.6.2-py3-none-any.whl --no-deps","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-03T01:32:15.844896Z","iopub.execute_input":"2023-11-03T01:32:15.845264Z","iopub.status.idle":"2023-11-03T01:33:03.106115Z","shell.execute_reply.started":"2023-11-03T01:32:15.845213Z","shell.execute_reply":"2023-11-03T01:33:03.104805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìö | Import Libraries ","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"torch\"  # or \"tensorflow\" or \"torch\"\n\nimport keras_nlp\nimport keras_core as keras \nimport keras_core.backend as K\n\n\nimport jax\nimport tensorflow as tf\n# from tensorflow import keras\n# import tensorflow.keras.backend as K\n\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport gc","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-03T01:33:03.108468Z","iopub.execute_input":"2023-11-03T01:33:03.108798Z","iopub.status.idle":"2023-11-03T01:33:16.609984Z","shell.execute_reply.started":"2023-11-03T01:33:03.108769Z","shell.execute_reply":"2023-11-03T01:33:16.609177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Library Version","metadata":{}},{"cell_type":"code","source":"print(\"TensorFlow:\", tf.__version__)\n# print(\"JAX:\", jax.__version__)\nprint(\"Keras:\", keras.__version__)\nprint(\"KerasNLP:\", keras_nlp.__version__)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-03T01:33:16.61113Z","iopub.execute_input":"2023-11-03T01:33:16.611652Z","iopub.status.idle":"2023-11-03T01:33:16.616769Z","shell.execute_reply.started":"2023-11-03T01:33:16.611625Z","shell.execute_reply":"2023-11-03T01:33:16.615935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚öôÔ∏è | Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    verbose = 0  # Verbosity\n    device = 'GPU'  # Device\n    seed = 42  # Random seed\n    batch_size = 6  # Batch size\n    drop_remainder = True  # Drop incomplete batches\n    ckpt_dir = \"/mnt/beegfs/xchen87/ai-gen/data/daigt-kerasnlp-ckpt\"  # Name of pretrained models\n    sequence_length = 200  # Input sequence length\n    class_names = ['real','fake']  # Class names [A, B, C, D, E]\n    num_classes = len(class_names)  # Number of classes\n    class_labels = list(range(num_classes))  # Class labels [0, 1, 2, 3, 4]\n    label2name = dict(zip(class_labels, class_names))  # Label to class name mapping\n    name2label = {v: k for k, v in label2name.items()}  # Class name to label mapping","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-03T01:33:16.619702Z","iopub.execute_input":"2023-11-03T01:33:16.619977Z","iopub.status.idle":"2023-11-03T01:33:16.636453Z","shell.execute_reply.started":"2023-11-03T01:33:16.619948Z","shell.execute_reply":"2023-11-03T01:33:16.635641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚ôªÔ∏è | Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-03T01:33:16.637422Z","iopub.execute_input":"2023-11-03T01:33:16.637699Z","iopub.status.idle":"2023-11-03T01:33:16.653097Z","shell.execute_reply.started":"2023-11-03T01:33:16.637675Z","shell.execute_reply":"2023-11-03T01:33:16.65216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üíæ | Hardware\nFollowing codes automatically detects hardware (TPU or GPU). ","metadata":{}},{"cell_type":"code","source":"def get_device():\n    \"Detect and intializes GPU/TPU automatically\"\n    try:\n        # Connect to TPU\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() \n        # Set TPU strategy\n        strategy = tf.distribute.TPUStrategy(tpu)\n        print(f'> Running on TPU', tpu.master(), end=' | ')\n        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n        device=CFG.device\n    except:\n        # If TPU is not available, detect GPUs\n        gpus = tf.config.list_logical_devices('GPU')\n        ngpu = len(gpus)\n         # Check number of GPUs\n        if ngpu:\n            # Set GPU strategy\n            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n            # Print GPU details\n            print(\"> Running on GPU\", end=' | ')\n            print(\"Num of GPUs: \", ngpu)\n            device='GPU'\n        else:\n            # If no GPUs are available, use CPU\n            print(\"> Running on CPU\")\n            strategy = tf.distribute.get_strategy()\n            device='CPU'\n    return strategy, device","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-03T01:33:16.654211Z","iopub.execute_input":"2023-11-03T01:33:16.654458Z","iopub.status.idle":"2023-11-03T01:33:16.662026Z","shell.execute_reply.started":"2023-11-03T01:33:16.654437Z","shell.execute_reply":"2023-11-03T01:33:16.661293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize GPU/TPU/TPU-VM\nstrategy, CFG.device = get_device()\nCFG.replicas = strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2023-11-03T01:33:16.662875Z","iopub.execute_input":"2023-11-03T01:33:16.663112Z","iopub.status.idle":"2023-11-03T01:33:22.537559Z","shell.execute_reply.started":"2023-11-03T01:33:16.663091Z","shell.execute_reply":"2023-11-03T01:33:22.53663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìÅ | Dataset Path ","metadata":{}},{"cell_type":"code","source":"BASE_PATH = '/mnt/beegfs/xchen87/ai-gen/data/llm-detect-ai-generated-text'","metadata":{"execution":{"iopub.status.busy":"2023-11-03T01:33:22.539213Z","iopub.execute_input":"2023-11-03T01:33:22.539581Z","iopub.status.idle":"2023-11-03T01:33:22.543925Z","shell.execute_reply.started":"2023-11-03T01:33:22.539546Z","shell.execute_reply":"2023-11-03T01:33:22.542981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìñ | Meta Data \n* `{test|train}_essays.csv`\n    * `id` - A unique identifier for each essay.\n    * `prompt_id` - Identifies the prompt the essay was written in response to.\n    * `text` - The essay text itself.\n    * `generated` - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays.csv.\n* **sample_submission.csv** - is the valid sample submission.","metadata":{}},{"cell_type":"markdown","source":"## Test Data","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(f'{BASE_PATH}/test_essays.csv')  # Read CSV file into a DataFrame\n\n# Display information about the train data\nprint(\"# Test Data: {:,}\".format(len(test_df)))\nprint(\"# Sample:\")\ndisplay(test_df.head(2))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-03T01:33:22.545338Z","iopub.execute_input":"2023-11-03T01:33:22.545907Z","iopub.status.idle":"2023-11-03T01:33:22.583165Z","shell.execute_reply.started":"2023-11-03T01:33:22.545872Z","shell.execute_reply":"2023-11-03T01:33:22.582313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üçΩÔ∏è | Preprocessing\n\n**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n\n**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n\nExplore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)","metadata":{}},{"cell_type":"code","source":"vocab_path = '/mnt/beegfs/xchen87/ai-gen/data/keras-nlp-deberta-v3-base-en-vocab-ds/vocab.spm'\ntokenizer= keras_nlp.models.DebertaV3Tokenizer(vocab_path)\npreprocessor= keras_nlp.models.DebertaV3Preprocessor(tokenizer, sequence_length=CFG.sequence_length)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T01:33:22.587042Z","iopub.execute_input":"2023-11-03T01:33:22.587556Z","iopub.status.idle":"2023-11-03T01:33:23.363121Z","shell.execute_reply.started":"2023-11-03T01:33:22.587529Z","shell.execute_reply":"2023-11-03T01:33:23.362278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's examine what the output shape of the preprocessing layer looks like. The output shape of the layer can be represented as $(num\\_choices, sequence\\_length)$.","metadata":{}},{"cell_type":"code","source":"outs = preprocessor(test_df.text.iloc[0])  # Process options for the first row\n\n# Display the shape of each processed output\nfor k, v in outs.items():\n    print(k, \":\", v.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T01:33:23.364483Z","iopub.execute_input":"2023-11-03T01:33:23.364807Z","iopub.status.idle":"2023-11-03T01:33:23.649081Z","shell.execute_reply.started":"2023-11-03T01:33:23.364781Z","shell.execute_reply":"2023-11-03T01:33:23.648089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use the `preprocessing_fn` function to transform each text option using the `dataset.map(preprocessing_fn)` method.","metadata":{}},{"cell_type":"code","source":"def preprocess_fn(text, label=None):\n    text = preprocessor(text)  # Preprocess text\n    return (text, label) if label is not None else text  # Return processed text and label if available","metadata":{"execution":{"iopub.status.busy":"2023-11-03T01:33:23.650451Z","iopub.execute_input":"2023-11-03T01:33:23.650811Z","iopub.status.idle":"2023-11-03T01:33:23.656057Z","shell.execute_reply.started":"2023-11-03T01:33:23.650778Z","shell.execute_reply":"2023-11-03T01:33:23.655184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üçö | DataLoader\n\nThe code below sets up a robust data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences.\n\nTo learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data).","metadata":{}},{"cell_type":"code","source":"def build_dataset(texts, labels=None, batch_size=32,\n                  cache=False, drop_remainder=True,\n                  augment=False, repeat=False, shuffle=1024):\n    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n    slices = (texts,) if labels is None else (texts, labels)  # Create slices\n    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function\n    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled\n    opt = tf.data.Options()  # Create dataset options\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n        opt.experimental_deterministic = False\n    ds = ds.with_options(opt)  # Set dataset options\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # Batch dataset\n    ds = ds.prefetch(AUTO)  # Prefetch next batch\n    return ds  # Return the built dataset","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-03T01:33:23.65714Z","iopub.execute_input":"2023-11-03T01:33:23.657436Z","iopub.status.idle":"2023-11-03T01:33:23.672707Z","shell.execute_reply.started":"2023-11-03T01:33:23.657413Z","shell.execute_reply":"2023-11-03T01:33:23.671759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fetch Train/test Dataset\n\nThe function below generates the training and testation datasets for a given fold.","metadata":{}},{"cell_type":"code","source":"def get_test_dataset(test_df):\n    test_texts = test_df.text.tolist()  # Extract testation texts\n    \n    # Build testation dataset\n    test_ds = build_dataset(test_texts, labels=None,\n                             batch_size=min(CFG.batch_size*CFG.replicas, len(test_df)), cache=False,\n                             shuffle=False, drop_remainder=False, repeat=False)\n    \n    return test_ds  # Return datasets and dataframes","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-11-03T01:33:23.673838Z","iopub.execute_input":"2023-11-03T01:33:23.67408Z","iopub.status.idle":"2023-11-03T01:33:23.684534Z","shell.execute_reply.started":"2023-11-03T01:33:23.674059Z","shell.execute_reply":"2023-11-03T01:33:23.683695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ü§ñ | Modeling\n\n","metadata":{}},{"cell_type":"code","source":"def build_model():\n    # Create a DebertaV3Classifier model\n    classifier = keras_nlp.models.DebertaV3Classifier.from_preset(\n        CFG.preset,\n        load_weights=False,\n        preprocessor=None,\n        num_classes=1 # one output per one option, for five options total 5 outputs\n    )\n    inputs = classifier.input\n    logits = classifier(inputs)\n        \n    # Compute final output\n    outputs = keras.layers.Activation(\"sigmoid\")(logits)\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-03T01:33:23.685442Z","iopub.execute_input":"2023-11-03T01:33:23.685728Z","iopub.status.idle":"2023-11-03T01:33:23.695745Z","shell.execute_reply.started":"2023-11-03T01:33:23.685706Z","shell.execute_reply":"2023-11-03T01:33:23.69503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ckpt processing\nFor some reason, `keras.models.load_model` requires write access as `/mnt/beegfs/xchen87/ai-gen/data` doesn't have that access it throws error. Workaround is to simply copy the `ckpts` to other directory then load the model.","metadata":{}},{"cell_type":"code","source":"# Get the checkpoint directory and name\nckpt_dir = CFG.ckpt_dir\nckpt_name = ckpt_dir.split('/')[3]\n\n# Copy the checkpoints to a new directory in the /kaggle directory\n!cp -r {ckpt_dir} /kaggle/{ckpt_name}\n\n# List all the checkpoint paths in the new directory\nnew_ckpt_dir = f\"/kaggle/{ckpt_name}\"\nckpt_paths = glob(os.path.join(new_ckpt_dir, '*.keras'))\n\nprint(\"Total CKPT:\", len(ckpt_paths))","metadata":{"execution":{"iopub.status.busy":"2023-11-03T01:33:23.696759Z","iopub.execute_input":"2023-11-03T01:33:23.697072Z","iopub.status.idle":"2023-11-03T01:33:51.729768Z","shell.execute_reply.started":"2023-11-03T01:33:23.697041Z","shell.execute_reply":"2023-11-03T01:33:51.728673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üß™ | Prediction","metadata":{}},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# Initialize an array to store predictions for each fold\nfold_preds = np.zeros(shape=(len(test_df),), dtype='float32')\n\n# # Build model\n# model = build_model()\n\n# Iterate through each checkpoint path\nfor ckpt_path in tqdm(ckpt_paths):\n    # Load the pre-trained model from the checkpoint\n    model = keras.models.load_model(\n        ckpt_path,\n        compile=False,\n    )\n#     model.load_weights(ckpt_path)\n    \n    # Get the test dataset\n    test_ds = get_test_dataset(test_df)\n    \n    # Generate predictions using the model\n    preds = model.predict(\n        test_ds,\n        batch_size=min(CFG.batch_size * CFG.replicas * 2, len(test_df)),  # Set batch size\n        verbose=1\n    )\n    \n    # Add predictions to fold_preds and average over checkpoints\n    fold_preds += preds.squeeze() / len(ckpt_paths)\n    \n    # Clean up by deleting the model and collecting garbage\n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-03T01:33:51.73135Z","iopub.execute_input":"2023-11-03T01:33:51.73167Z","iopub.status.idle":"2023-11-03T01:39:08.176925Z","shell.execute_reply.started":"2023-11-03T01:33:51.731639Z","shell.execute_reply":"2023-11-03T01:39:08.175993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Prediction","metadata":{}},{"cell_type":"code","source":"# Format predictions and true answers\npred_answers = (fold_preds > 0.5).astype(int).squeeze()\n\n# Check 5 Predictions\nprint(\"# Predictions\\n\")\nfor i in range(3):\n    row = test_df.iloc[i]\n    text  = row.text\n    pred_answer = CFG.label2name[pred_answers[i]]\n    print(f\"‚ùì Text {i+1}:\\n{text}\\n\")\n    print(f\"ü§ñ Predicted: {pred_answer}\\n\")\n    print(\"-\"*90, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-11-03T01:39:08.178433Z","iopub.execute_input":"2023-11-03T01:39:08.178727Z","iopub.status.idle":"2023-11-03T01:39:08.186585Z","shell.execute_reply.started":"2023-11-03T01:39:08.178701Z","shell.execute_reply":"2023-11-03T01:39:08.185663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìÆ | Submission","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame to store the submission\nsub_df = test_df[[\"id\"]].copy()\n\n# Add the formatted predictions to the submission DataFrame\nsub_df[\"generated\"] = fold_preds.squeeze()\n\n# Save Submission\nsub_df.to_csv('submission.csv',index=False)\n\n# Display the first 2 rows of the submission DataFrame\nsub_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T01:39:08.18786Z","iopub.execute_input":"2023-11-03T01:39:08.188208Z","iopub.status.idle":"2023-11-03T01:39:08.220218Z","shell.execute_reply.started":"2023-11-03T01:39:08.188172Z","shell.execute_reply":"2023-11-03T01:39:08.219291Z"},"trusted":true},"execution_count":null,"outputs":[]}]}