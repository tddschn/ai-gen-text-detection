#!/usr/bin/env python
# coding: utf-8

# # LLM - Detect AI Generated Text
# > Identify which essay was written by a large language model
# 
# <img src="https://user-images.githubusercontent.com/36858976/279902422-b365f6ef-ef01-49ac-af7f-0bc2ca3ba835.png">

# # üéØ | Motivation
# 
# * In this notebook, we will demonstrate the usage of the multi-backend capabilities of `KerasCore` and `KerasNLP` for the **Detect Fake Text** infernece.

# # üìì | Notebooks
# 
# * Train: [Detect Fake Text: KerasNLP [TF/Torch/JAX][Train]](https://www.kaggle.com/code/awsaf49/detect-fake-text-kerasnlp-tf-torch-jax-train)
# * Infer: [Detect Fake Text: KerasNLP [TF/Torch/JAX][Infer]](https://www.kaggle.com/code/awsaf49/detect-fake-text-kerasnlp-tf-torch-jax-infer)

# # üõ† | Install Libraries 

# In[ ]:


!pip install /mnt/beegfs/xchen87/ai-gen/data/llm-science-exam-lib-ds/keras_core-0.1.7-py3-none-any.whl --no-deps
!pip install /mnt/beegfs/xchen87/ai-gen/data/llm-science-exam-lib-ds/keras_nlp-0.6.2-py3-none-any.whl --no-deps

# # üìö | Import Libraries 

# In[ ]:


import os
os.environ["KERAS_BACKEND"] = "torch"  # or "tensorflow" or "torch"

import keras_nlp
import keras_core as keras 
import keras_core.backend as K


import jax
import tensorflow as tf
# from tensorflow import keras
# import tensorflow.keras.backend as K

import numpy as np 
import pandas as pd

import matplotlib.pyplot as plt

from glob import glob
from tqdm.notebook import tqdm
import gc

# ## Library Version

# In[ ]:


print("TensorFlow:", tf.__version__)
# print("JAX:", jax.__version__)
print("Keras:", keras.__version__)
print("KerasNLP:", keras_nlp.__version__)

# # ‚öôÔ∏è | Configuration

# In[ ]:


class CFG:
    verbose = 0  # Verbosity
    device = 'GPU'  # Device
    seed = 42  # Random seed
    batch_size = 6  # Batch size
    drop_remainder = True  # Drop incomplete batches
    ckpt_dir = "/mnt/beegfs/xchen87/ai-gen/data/daigt-kerasnlp-ckpt"  # Name of pretrained models
    sequence_length = 200  # Input sequence length
    class_names = ['real','fake']  # Class names [A, B, C, D, E]
    num_classes = len(class_names)  # Number of classes
    class_labels = list(range(num_classes))  # Class labels [0, 1, 2, 3, 4]
    label2name = dict(zip(class_labels, class_names))  # Label to class name mapping
    name2label = {v: k for k, v in label2name.items()}  # Class name to label mapping

# # ‚ôªÔ∏è | Reproducibility 
# Sets value for random seed to produce similar result in each run.

# In[ ]:


keras.utils.set_random_seed(CFG.seed)

# # üíæ | Hardware
# Following codes automatically detects hardware (TPU or GPU). 

# In[ ]:


def get_device():
    "Detect and intializes GPU/TPU automatically"
    try:
        # Connect to TPU
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() 
        # Set TPU strategy
        strategy = tf.distribute.TPUStrategy(tpu)
        print(f'> Running on TPU', tpu.master(), end=' | ')
        print('Num of TPUs: ', strategy.num_replicas_in_sync)
        device=CFG.device
    except:
        # If TPU is not available, detect GPUs
        gpus = tf.config.list_logical_devices('GPU')
        ngpu = len(gpus)
         # Check number of GPUs
        if ngpu:
            # Set GPU strategy
            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU
            # Print GPU details
            print("> Running on GPU", end=' | ')
            print("Num of GPUs: ", ngpu)
            device='GPU'
        else:
            # If no GPUs are available, use CPU
            print("> Running on CPU")
            strategy = tf.distribute.get_strategy()
            device='CPU'
    return strategy, device

# In[ ]:


# Initialize GPU/TPU/TPU-VM
strategy, CFG.device = get_device()
CFG.replicas = strategy.num_replicas_in_sync

# # üìÅ | Dataset Path 

# In[ ]:


BASE_PATH = '/mnt/beegfs/xchen87/ai-gen/data/llm-detect-ai-generated-text'

# # üìñ | Meta Data 
# * `{test|train}_essays.csv`
#     * `id` - A unique identifier for each essay.
#     * `prompt_id` - Identifies the prompt the essay was written in response to.
#     * `text` - The essay text itself.
#     * `generated` - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays.csv.
# * **sample_submission.csv** - is the valid sample submission.

# ## Test Data

# In[ ]:


test_df = pd.read_csv(f'{BASE_PATH}/test_essays.csv')  # Read CSV file into a DataFrame

# Display information about the train data
print("# Test Data: {:,}".format(len(test_df)))
print("# Sample:")
display(test_df.head(2))

# # üçΩÔ∏è | Preprocessing
# 
# **What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.
# 
# **Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `"The quick brown fox"` into `["the", "qu", "##ick", "br", "##own", "fox"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.
# 
# Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:
# - [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)
# - [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)

# In[ ]:


vocab_path = '/mnt/beegfs/xchen87/ai-gen/data/keras-nlp-deberta-v3-base-en-vocab-ds/vocab.spm'
tokenizer= keras_nlp.models.DebertaV3Tokenizer(vocab_path)
preprocessor= keras_nlp.models.DebertaV3Preprocessor(tokenizer, sequence_length=CFG.sequence_length)

# Now, let's examine what the output shape of the preprocessing layer looks like. The output shape of the layer can be represented as $(num\_choices, sequence\_length)$.

# In[ ]:


outs = preprocessor(test_df.text.iloc[0])  # Process options for the first row

# Display the shape of each processed output
for k, v in outs.items():
    print(k, ":", v.shape)

# We'll use the `preprocessing_fn` function to transform each text option using the `dataset.map(preprocessing_fn)` method.

# In[ ]:


def preprocess_fn(text, label=None):
    text = preprocessor(text)  # Preprocess text
    return (text, label) if label is not None else text  # Return processed text and label if available

# # üçö | DataLoader
# 
# The code below sets up a robust data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences.
# 
# To learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data).

# In[ ]:


def build_dataset(texts, labels=None, batch_size=32,
                  cache=False, drop_remainder=True,
                  augment=False, repeat=False, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option
    slices = (texts,) if labels is None else (texts, labels)  # Create slices
    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices
    ds = ds.cache() if cache else ds  # Cache dataset if enabled
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function
    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled
    opt = tf.data.Options()  # Create dataset options
    if shuffle: 
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)  # Set dataset options
    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # Batch dataset
    ds = ds.prefetch(AUTO)  # Prefetch next batch
    return ds  # Return the built dataset

# ## Fetch Train/test Dataset
# 
# The function below generates the training and testation datasets for a given fold.

# In[ ]:


def get_test_dataset(test_df):
    test_texts = test_df.text.tolist()  # Extract testation texts
    
    # Build testation dataset
    test_ds = build_dataset(test_texts, labels=None,
                             batch_size=min(CFG.batch_size*CFG.replicas, len(test_df)), cache=False,
                             shuffle=False, drop_remainder=False, repeat=False)
    
    return test_ds  # Return datasets and dataframes

# # ü§ñ | Modeling
# 
# 

# In[ ]:


def build_model():
    # Create a DebertaV3Classifier model
    classifier = keras_nlp.models.DebertaV3Classifier.from_preset(
        CFG.preset,
        load_weights=False,
        preprocessor=None,
        num_classes=1 # one output per one option, for five options total 5 outputs
    )
    inputs = classifier.input
    logits = classifier(inputs)
        
    # Compute final output
    outputs = keras.layers.Activation("sigmoid")(logits)
    model = keras.Model(inputs, outputs)
    return model

# ## Ckpt processing
# For some reason, `keras.models.load_model` requires write access as `/mnt/beegfs/xchen87/ai-gen/data` doesn't have that access it throws error. Workaround is to simply copy the `ckpts` to other directory then load the model.

# In[ ]:


# Get the checkpoint directory and name
ckpt_dir = CFG.ckpt_dir
ckpt_name = ckpt_dir.split('/')[3]

# Copy the checkpoints to a new directory in the /kaggle directory
!cp -r {ckpt_dir} /kaggle/{ckpt_name}

# List all the checkpoint paths in the new directory
new_ckpt_dir = f"/kaggle/{ckpt_name}"
ckpt_paths = glob(os.path.join(new_ckpt_dir, '*.keras'))

print("Total CKPT:", len(ckpt_paths))

# # üß™ | Prediction

# ## Inference

# In[ ]:


# Initialize an array to store predictions for each fold
fold_preds = np.zeros(shape=(len(test_df),), dtype='float32')

# # Build model
# model = build_model()

# Iterate through each checkpoint path
for ckpt_path in tqdm(ckpt_paths):
    # Load the pre-trained model from the checkpoint
    model = keras.models.load_model(
        ckpt_path,
        compile=False,
    )
#     model.load_weights(ckpt_path)
    
    # Get the test dataset
    test_ds = get_test_dataset(test_df)
    
    # Generate predictions using the model
    preds = model.predict(
        test_ds,
        batch_size=min(CFG.batch_size * CFG.replicas * 2, len(test_df)),  # Set batch size
        verbose=1
    )
    
    # Add predictions to fold_preds and average over checkpoints
    fold_preds += preds.squeeze() / len(ckpt_paths)
    
    # Clean up by deleting the model and collecting garbage
    del model
    gc.collect()

# ## Check Prediction

# In[ ]:


# Format predictions and true answers
pred_answers = (fold_preds > 0.5).astype(int).squeeze()

# Check 5 Predictions
print("# Predictions\n")
for i in range(3):
    row = test_df.iloc[i]
    text  = row.text
    pred_answer = CFG.label2name[pred_answers[i]]
    print(f"‚ùì Text {i+1}:\n{text}\n")
    print(f"ü§ñ Predicted: {pred_answer}\n")
    print("-"*90, "\n")

# # üìÆ | Submission

# In[ ]:


# Create a DataFrame to store the submission
sub_df = test_df[["id"]].copy()

# Add the formatted predictions to the submission DataFrame
sub_df["generated"] = fold_preds.squeeze()

# Save Submission
sub_df.to_csv('submission.csv',index=False)

# Display the first 2 rows of the submission DataFrame
sub_df.head(2)
