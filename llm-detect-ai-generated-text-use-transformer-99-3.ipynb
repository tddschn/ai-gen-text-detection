{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6851361,"sourceType":"datasetVersion","datasetId":3938197},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7082713,"sourceType":"datasetVersion","datasetId":4039374},{"sourceId":7246053,"sourceType":"datasetVersion","datasetId":4197568},{"sourceId":7264407,"sourceType":"datasetVersion","datasetId":3954249,"isSourceIdPinned":true},{"sourceId":7456307,"sourceType":"datasetVersion","datasetId":4050356},{"sourceId":159219517,"sourceType":"kernelVersion"}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":5479.910387,"end_time":"2024-01-22T21:13:39.058175","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-22T19:42:19.147788","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0037d8b88832465a9dc0ab6504706b2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b129bad7f8364d81b15598e444d2c3c0","IPY_MODEL_45edc3618aeb4beea06a72edc5ea515f","IPY_MODEL_ca5133e4bd6d483eb61dad80375fc525"],"layout":"IPY_MODEL_401a60f7ba464c91b667ff9f5f2be64f"}},"052926adc0dd45dc8021fa32bc614d92":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"070630fb3d604759898ef3408446a069":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42087aaf6236468a8377763002fbbfda","placeholder":"​","style":"IPY_MODEL_e6f3d4eec37647a389b11e69dff125ea","value":" 1/1 [00:00&lt;00:00, 60.14ba/s]"}},"097284aefd44414990334cc1df02686a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a12659a772a450f9ee16f0de62c6bcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1be941643480426086123c0458130cae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db32580616354183b76b4e30f3217c64","placeholder":"​","style":"IPY_MODEL_68e86e84cfe940f08c2de0d4dc644e5b","value":" 24/24 [00:17&lt;00:00,  1.10ba/s]"}},"210b2e491ec14017a28199f82a3df829":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f40cb9090a224b788a6bdbdb8f606aea","IPY_MODEL_bff5e49e1cab4559b7939baed1717604","IPY_MODEL_1be941643480426086123c0458130cae"],"layout":"IPY_MODEL_df67fd2a70554b149daa7f9d8b6ae281"}},"230dd42e737d41088a503e8a9aa0ce41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"233aa42549f94941b7e5eef99239260f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad28dc17444b4a4886f5284f8766dde8","IPY_MODEL_5df0f825a96e4303857ce46e7ca5468b","IPY_MODEL_070630fb3d604759898ef3408446a069"],"layout":"IPY_MODEL_7e0444283d454efda052632c6d9be063"}},"295fcc98adea41efa0b718fa59a84c52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_052926adc0dd45dc8021fa32bc614d92","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_439510ce7f514dca8131e56608202efe","value":1}},"2bb3390fdc204d7aa75d62c852efc6c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e6ce6af7a6541818982a78da215d127","placeholder":"​","style":"IPY_MODEL_9f41c63c92f34848852b3a9f9b176e21","value":" 1/1 [00:00&lt;00:00, 70.69ba/s]"}},"2d789b3e3b1d4a08ba321aace3fe5275":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33a675a6cee148b0906e264b67de06f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a435557e5994e698838f7d2f6846b28","placeholder":"​","style":"IPY_MODEL_230dd42e737d41088a503e8a9aa0ce41","value":"100%"}},"401a60f7ba464c91b667ff9f5f2be64f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42087aaf6236468a8377763002fbbfda":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"439510ce7f514dca8131e56608202efe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45edc3618aeb4beea06a72edc5ea515f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bb13980c66442c6b0638ee9f21c46ce","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d789b3e3b1d4a08ba321aace3fe5275","value":1}},"4e6ce6af7a6541818982a78da215d127":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57e719497e1b4f76bcba507ce266d803":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5df0f825a96e4303857ce46e7ca5468b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be2e93e6dacb482cb6c50be8640e3c8d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a12659a772a450f9ee16f0de62c6bcc","value":1}},"60a313190edb4226a027242544c482a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68e86e84cfe940f08c2de0d4dc644e5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bb13980c66442c6b0638ee9f21c46ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"783906b388e742a483b7c8e23a82a9ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33a675a6cee148b0906e264b67de06f9","IPY_MODEL_295fcc98adea41efa0b718fa59a84c52","IPY_MODEL_2bb3390fdc204d7aa75d62c852efc6c7"],"layout":"IPY_MODEL_88c2d529ab7d4f2ab49d72a7be475be7"}},"7e0444283d454efda052632c6d9be063":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88c2d529ab7d4f2ab49d72a7be475be7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a435557e5994e698838f7d2f6846b28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9892d446c7de41e885832dbe49001b37":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f41c63c92f34848852b3a9f9b176e21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acd308a043db44efae976e352c7d81c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad28dc17444b4a4886f5284f8766dde8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7f9b3a77c1045e0814ad6ac20d91a5a","placeholder":"​","style":"IPY_MODEL_57e719497e1b4f76bcba507ce266d803","value":"100%"}},"b129bad7f8364d81b15598e444d2c3c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9892d446c7de41e885832dbe49001b37","placeholder":"​","style":"IPY_MODEL_acd308a043db44efae976e352c7d81c8","value":"100%"}},"b449a27c1f7144fa8395df9da31a17fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be2e93e6dacb482cb6c50be8640e3c8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bff5e49e1cab4559b7939baed1717604":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b449a27c1f7144fa8395df9da31a17fe","max":24,"min":0,"orientation":"horizontal","style":"IPY_MODEL_097284aefd44414990334cc1df02686a","value":24}},"ca5133e4bd6d483eb61dad80375fc525":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe1d0e13038947cfa8768afe6723d240","placeholder":"​","style":"IPY_MODEL_60a313190edb4226a027242544c482a4","value":" 1/1 [00:00&lt;00:00, 16.66ba/s]"}},"db32580616354183b76b4e30f3217c64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd3049c178584f0caaafc2e4cd81214a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df67fd2a70554b149daa7f9d8b6ae281":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6f3d4eec37647a389b11e69dff125ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f40cb9090a224b788a6bdbdb8f606aea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd3049c178584f0caaafc2e4cd81214a","placeholder":"​","style":"IPY_MODEL_fc9721866bf140fbafb69b0b79dc1263","value":"100%"}},"f7f9b3a77c1045e0814ad6ac20d91a5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc9721866bf140fbafb69b0b79dc1263":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe1d0e13038947cfa8768afe6723d240":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#install libs\nimport pickle\nfrom leven_search import LevenSearch, EditCost, EditCostConfig, GranularEditCostConfig\n\nwith open('/kaggle/usr/lib/install-levenshtein-search-library/leven_search.pkl', 'rb') as file:\n    lev_search = pickle.load(file)\n","metadata":{"papermill":{"duration":1.450582,"end_time":"2024-01-22T19:42:23.930627","exception":false,"start_time":"2024-01-22T19:42:22.480045","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-29T21:28:07.484146Z","iopub.execute_input":"2024-01-29T21:28:07.484753Z","iopub.status.idle":"2024-01-29T21:28:10.933544Z","shell.execute_reply.started":"2024-01-29T21:28:07.484721Z","shell.execute_reply":"2024-01-29T21:28:10.932739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Add all imports \nimport os\nimport gc\nimport re\nimport sys\nimport torch\nimport datasets\nimport transformers\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, PreTrainedTokenizerFast\nfrom tokenizers import ( decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer,)\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom collections import Counter\nfrom itertools import chain\n\nfrom tqdm.auto import tqdm\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\n\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.linear_model import LinearRegression  #0.925\n# from sklearn.linear_model import SGDClassifier #0.42\nfrom sklearn.preprocessing import MaxAbsScaler\n# from sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import LogisticRegression\n\n\nfrom sklearn.svm import LinearSVR\nfrom scipy.sparse import vstack as spvstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import roc_auc_score\nfrom transformers import EarlyStoppingCallback\nfrom collections import defaultdict\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\n#disable wandb\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":18.126458,"end_time":"2024-01-22T19:42:42.063855","exception":false,"start_time":"2024-01-22T19:42:23.937397","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-29T21:28:10.935059Z","iopub.execute_input":"2024-01-29T21:28:10.935315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#spell checker code \ndef sentence_correcter(text):\n    wrong_words = []\n    correct_words = dict()\n\n    word_list = re.findall(r'\\b\\w+\\b|[.,\\s]', text)\n\n    for t in word_list:\n        correct_word = t\n\n        if len(t)>2:\n            result = lev_search.find_dist(t, max_distance=0)\n            result = list(result.__dict__['words'].values())\n\n            if len(result) == 0:\n                result = lev_search.find_dist(t, max_distance=1)\n                result = list(result.__dict__['words'].values())\n                if len(result):\n                    correct_word = result[0].word\n                    wrong_words.append((t, result))\n\n        correct_words[t] = correct_word\n\n    dict_freq = defaultdict(lambda :0)           \n    for wrong_word in wrong_words:\n        _, result = wrong_word\n\n        for res in result:\n            updates = res.updates\n            parts = str(updates[0]).split(\" -> \")\n            if len(parts) == 2:\n                from_char = parts[0]\n                to_char = parts[1]\n                dict_freq[(from_char, to_char)] += 1\n\n    if len(dict_freq):\n        max_key = max(dict_freq, key=dict_freq.get)\n        count = dict_freq[max_key]\n    else:\n        count = 0\n\n    if count > 0.06*len(text.split()):\n        gec = GranularEditCostConfig(default_cost=10, edit_costs=[EditCost(max_key[0], max_key[1], 1)])\n\n        for wrong_word in wrong_words:\n            word, _ = wrong_word\n            result = lev_search.find_dist(word, max_distance=9, edit_cost_config=gec)\n            result = list(result.__dict__['words'].values())\n            if len(result):\n                correct_words[word] = result[0].word\n            else:\n                correct_word = word\n\n\n    correct_sentence = []\n    for t in word_list:\n        correct_sentence.append(correct_words[t])\n\n    return \"\".join(correct_sentence)","metadata":{"papermill":{"duration":0.021336,"end_time":"2024-01-22T19:42:42.091905","exception":false,"start_time":"2024-01-22T19:42:42.070569","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define general use methods \n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))  \n\n\nmodel_checkpoint_base = \"/kaggle/input/distilroberta-base/distilroberta-base\"\nmodel_checkpoint_infer = \"/kaggle/input/detect-llm-models/distilroberta-finetuned_v5/checkpoint-13542\"\n\ntokenizer_infer = AutoTokenizer.from_pretrained(model_checkpoint_infer)\ntokenizer_train = AutoTokenizer.from_pretrained(model_checkpoint_base)\n","metadata":{"papermill":{"duration":0.370938,"end_time":"2024-01-22T19:42:42.469474","exception":false,"start_time":"2024-01-22T19:42:42.098536","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define data reading methods \n\ndef read_sub():\n    return pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n\ndef read_test():\n        return pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n\ndef read_dummy_test():\n    t =  pd.read_csv('/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv').tail(9000)\n    t['id'] = range(0, len(t))\n    t = t[['id', 'text']]\n    return t\n\n\ndef append_train_from_sub_phase(org_train_data, train_from_sub):\n    \n    train_from_sub.drop('generated', axis=1, inplace=True)\n    train_from_sub.reset_index(drop=True, inplace=True)\n\n    train_from_sub = train_from_sub[['text', 'label']]\n    \n    train =  pd.concat([org_train_data, train_from_sub])\n    \n    return train\n","metadata":{"papermill":{"duration":0.016075,"end_time":"2024-01-22T19:42:42.492627","exception":false,"start_time":"2024-01-22T19:42:42.476552","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define readig for text categorization\ndef filter_dataframe(df, category):\n    # Filter the DataFrame for the specified category or NaN in 'prompt_name'\n    filtered_df = df[(df['prompt_name'] == category) | (df['prompt_name'].isna())]\n    return filtered_df\n\ndef filter_dataframe_single_category(df, category):\n    # Filter the DataFrame for the specified category in 'prompt_name'\n    filtered_df = df[df['prompt_name'] == category]\n    return filtered_df\n\ndef standardize_categories(df):\n    # Standardize the category name\n    df['prompt_name'] = df['prompt_name'].str.replace('\"A Cowboy Who Rode the Waves\"', 'A Cowboy Who Rode the Waves', regex=False)\n    return df\n\ndef assign_category(row):\n    if row['prompt_id'] == 1:\n        return \"Does the electoral college work?\"\n    elif row['prompt_id'] == 0:\n        return \"Car-free cities\"\n    else:\n        return None  # or some default value\n\n    \ndef read_train_all():\n    train = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n    train = train[['text', 'prompt_name', 'label']]\n    train = standardize_categories(train)\n\n    train_old =  pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\n    train_old.rename(columns={'generated': 'label'}, inplace=True)\n    train_old['prompt_name'] = train_old.apply(assign_category, axis=1)\n    train_old = train_old[['text', 'prompt_name', 'label']]\n    \n    lm_7b =  pd.read_csv(\"/kaggle/input/llm-mistral-7b-instruct-texts/Mistral7B_CME_v7.csv\")\n    lm_ali_1 =  pd.read_csv(\"/kaggle/input/llm-dataset/gen_llm_fac_v1.csv\")\n    #lm_ali_2 =  pd.read_csv(\"/kaggle/input/llm-dataset/gen_llm_elec_v1.csv\")\n    #lm_ali_3 =  pd.read_csv(\"/kaggle/input/llm-dataset/gen_llm_car_free_v1.csv\")\n    lm_ali_4 =  pd.read_csv(\"/kaggle/input/llm-dataset/gen_llm_exploring_venus_v1.csv\")\n    lm_ali_5 =  pd.read_csv(\"/kaggle/input/llm-dataset/gen_llm_face_on_mars_v1.csv\")\n    lm_ali_6 =  pd.read_csv(\"/kaggle/input/llm-dataset/gen_llm_driveless_cars_v1.csv\")\n    lm_ali_7 =  pd.read_csv(\"/kaggle/input/llm-dataset/gen_llm_cowboy_v1.csv\")\n    lm_ali_8 =  pd.read_csv(\"/kaggle/input/llm-dataset/gen_llm_cowboy_v2.csv\")\n    #lm_ali_9 =  pd.read_csv(\"/kaggle/input/llm-dataset/gen_llm_face_on_mars_v2.csv\")\n    gemini = pd.read_csv(\"/kaggle/input/gemini-pro-llm-daigt/gemini_pro_llm_text.csv\")\n    gemini = gemini[gemini['typos']==\"no\"]\n    #lm_data = pd.concat([lm_7b, lm_ali_1, lm_ali_2, lm_ali_3,lm_ali_4,lm_ali_5,lm_ali_6,lm_ali_7,lm_ali_8,lm_ali_9,gemini], ignore_index=True)\n    lm_data = pd.concat([lm_7b, lm_ali_1, lm_ali_4,lm_ali_5,lm_ali_6,lm_ali_7,lm_ali_8,lm_ali_8,gemini], ignore_index=True)\n\n    lm_data.rename(columns={'generated': 'label'}, inplace=True)\n    del gemini\n    gc.collect()\n    lm_data = lm_data[['text', 'prompt_name', 'label']]\n    lm_data = standardize_categories(lm_data)\n    train_old = standardize_categories(train_old)\n    \n    train =  pd.concat([train, lm_data, train_old])\n    return train, lm_data, train_old","metadata":{"papermill":{"duration":0.021723,"end_time":"2024-01-22T19:42:42.52093","exception":false,"start_time":"2024-01-22T19:42:42.499207","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define PBE Tokenizer class and related methods\n\nclass BPETokenizer:\n    ST = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n    \n    def __init__(\n        self,\n        vocab_size,\n    ):\n        self.vocab_size = vocab_size\n        self.tok = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n        self.tok.normalizer = normalizers.Sequence([normalizers.NFC()])\n        self.tok.pre_tokenizer = pre_tokenizers.ByteLevel()\n        \n    @classmethod\n    def chunk_dataset(cls, dataset, chunk_size=1_000):\n        for i in range(0, len(dataset), chunk_size):\n            yield dataset[i : i + chunk_size][\"text\"]\n        \n    def train(self, data):\n        trainer = trainers.BpeTrainer(vocab_size=self.vocab_size, special_tokens=self.ST)\n        dataset = Dataset.from_pandas(data[[\"text\"]])\n        self.tok.train_from_iterator(self.chunk_dataset(dataset), trainer=trainer)\n        return self\n    \n    def tokenize(self, data):\n        tokenized_texts = []\n        for text in tqdm(data['text'].tolist()):\n            tokenized_texts.append(self.tok.encode(text))\n        return tokenized_texts\n    \ndef train_tokenizer(train, lm_data, train_old, test):\n\n    tokenizer_train_data = pd.concat([lm_data,train_old])\n    tok_data = pd.concat([ tokenizer_train_data[[\"text\"]],  test[[\"text\"]] ]).reset_index(drop=True)\n    vc_counters = {}\n    for vs in [5_000]: # for if needed to train multi vocab_size \n        bpe_tok = BPETokenizer(vs).train(tok_data)\n        ctr = Counter(chain(*[x.ids for x in bpe_tok.tokenize(tok_data)]))\n        vc_counters[vs] = (bpe_tok, ctr)\n        tqdm.write(f\"completed tokenization with {vs:,} vocab size\")\n    return vc_counters\n\ndef tokenize_datasets (vc_counters, train, lm_data, test):\n    \n    bpe_tok = vc_counters[5_000][0]\n    test_extend = pd.concat([lm_data,test])\n    tokenized_texts_train = [x.tokens for x in bpe_tok.tokenize(train)]\n    tokenized_texts_test = [x.tokens for x in bpe_tok.tokenize(test)]\n    tokenized_texts_lm_data = [x.tokens for x in bpe_tok.tokenize(lm_data)]\n    tokenized_texts_test2 = tokenized_texts_lm_data + tokenized_texts_test\n    \n    del tokenized_texts_lm_data\n    gc.collect()\n    \n    return tokenized_texts_train, tokenized_texts_test, tokenized_texts_test2\n","metadata":{"papermill":{"duration":0.022579,"end_time":"2024-01-22T19:42:42.550507","exception":false,"start_time":"2024-01-22T19:42:42.527928","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def vectorizer related methods \ndef dummy1(text):\n    return text\n\ndef vectorizer_of_data(tokenized_texts_train,tokenized_texts_test,tokenized_texts_test2, min_diff, test):\n    len_test = len(test)\n    #print(len_test)\n    if len_test < 10:\n        min_diff = 0\n    \n    print(\"vectorizer - prepare vocab ..\")\n    vectorizer = TfidfVectorizer(ngram_range=(3, 6), lowercase=False, sublinear_tf=True, analyzer = 'word',min_df=min_diff, \n                                 tokenizer = dummy1, preprocessor = dummy1, token_pattern = None, strip_accents='unicode')\n    \n    vectorizer.fit(tokenized_texts_test2)\n    vocab = vectorizer.vocabulary_\n    \n    del vectorizer\n    gc.collect()\n    \n    vectorizer = TfidfVectorizer(ngram_range=(3, 6), lowercase=False, sublinear_tf=True, vocabulary=vocab, analyzer = 'word',\n                                 tokenizer = dummy1, preprocessor = dummy1, min_df=min_diff, token_pattern = None,\n                                 strip_accents='unicode')\n\n    print(\"vectorizer - fit transform on train ..\")\n\n    tf_train = vectorizer.fit_transform(tokenized_texts_train)\n    \n    print(\"vectorizer - transform on test ..\")\n\n    tf_test = vectorizer.transform(tokenized_texts_test)\n    \n    del tokenized_texts_test2\n    del tokenized_texts_test\n    del tokenized_texts_train\n    \n    del vectorizer\n    gc.collect()\n\n    return tf_train, tf_test","metadata":{"papermill":{"duration":0.016843,"end_time":"2024-01-22T19:42:42.573859","exception":false,"start_time":"2024-01-22T19:42:42.557016","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#methods to adopt learning from test data\n\ndef build_new_train_from_sub_all(final_preds_linear_tmp, test, X, Y):\n    \n    test.loc[:, 'generated'] = final_preds_linear_tmp\n    sorted_df = test.sort_values(by='generated', ascending=False)\n    top_rows = sorted_df.head(X).copy()\n    top_rows['label'] = 1\n    \n    # Select the bottom Y rows and set 'generated' to 0\n    bottom_rows = sorted_df.tail(Y).copy()\n    bottom_rows['label'] = 0\n    \n    # Concatenate the two subsets\n    train_from_sub = pd.concat([top_rows, bottom_rows])\n    \n    return train_from_sub\n\ndef build_new_train_from_sub_by_classs(final_preds_linear_tmp, test, X, Y):\n    test.loc[:, 'generated'] = final_preds_linear_tmp\n    class_dfs = {}\n    \n    # Iterate over each unique class value and create a separate DataFrame\n    for class_value in test['prompt_id'].unique():\n        class_dfs[class_value] = test[test['prompt_id'] == class_value]\n    #print(class_dfs[class_value])\n    \n    sorted_class_dfs = {class_value: df.sort_values(by='generated', ascending=False) for class_value, df in class_dfs.items()}\n    \n    new_class_dfs_with_generated = {}\n    \n    new_class_dfs_filtered = {}\n    for class_value, df in sorted_class_dfs.items():\n        if len(df) >= (X + Y):\n            top_rows = df.head(X).copy()\n            top_rows['label'] = 1\n            \n            bottom_rows = df.tail(Y).copy()\n            bottom_rows['label'] = 0\n            \n            combined = pd.concat([top_rows, bottom_rows], axis=0)\n            new_class_dfs_filtered[class_value] = combined\n            \n    if len(test) > 10:\n        train_from_sub = pd.concat(new_class_dfs_filtered.values(), ignore_index=True)\n    else:\n        train_from_sub = test\n        test['label'] = 1\n\n    return train_from_sub\n\n\ndef build_new_train_from_sub_add_all_data(final_preds_linear_tmp, test):\n    \n    test.loc[:, 'generated'] = final_preds_linear_tmp\n    median_label = test['generated'].median()\n    test['label'] = (test['generated'] >= median_label).astype(int)\n    train_from_sub = test.copy()\n    return train_from_sub\n","metadata":{"papermill":{"duration":0.020483,"end_time":"2024-01-22T19:42:42.600829","exception":false,"start_time":"2024-01-22T19:42:42.580346","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define linear models training and prediction methods\ndef MaxAbsScalerTransform(tf_train,tf_test):\n    scaler = MaxAbsScaler()\n    X_train_scaled = scaler.fit_transform(tf_train)\n    X_test_scaled = scaler.transform(tf_test)\n    return X_train_scaled, X_test_scaled\n    \n\ndef get_predictions_linear_LinearSVR(X_train_scaled,X_test_scaled, y_train):\n    \n    #X_train_scaled, X_test_scaled = MaxAbsScalerTransform(tf_train,tf_test, y_train)\n    \n    model = Ridge(solver='sag',max_iter=10000,tol=1e-4, alpha = 1)\n    model.fit(X_train_scaled, y_train)\n    preds = model.predict(X_test_scaled.copy())\n    preds = sigmoid(preds)\n    \n    return preds\n\ndef get_predictions_linear_Ridge(X_train_scaled,X_test_scaled, y_train):\n    \n    #X_train_scaled, X_test_scaled = MaxAbsScalerTransform(tf_train,tf_test, y_train)\n    \n    model = Ridge(solver='sag',max_iter=8000,tol=1e-4, alpha = 1)\n    model.fit(X_train_scaled, y_train)\n    preds = model.predict(X_test_scaled.copy())\n    preds = sigmoid(preds)\n    \n    return preds","metadata":{"papermill":{"duration":0.017565,"end_time":"2024-01-22T19:42:42.62503","exception":false,"start_time":"2024-01-22T19:42:42.607465","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define needed function and variables for transformer model used.\n\ndef preprocess_function_infer(examples):\n    return tokenizer_infer(examples['text'], max_length = 512 , padding=True, truncation=True)\n\ndef preprocess_function_train(examples):\n    return tokenizer_train(examples['text'], max_length=128, padding=True, truncation=True)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n    auc = roc_auc_score(labels, probs[:,1], multi_class='ovr')\n    return {\"roc_auc\": auc}\n\n","metadata":{"papermill":{"duration":0.016162,"end_time":"2024-01-22T19:42:42.647875","exception":false,"start_time":"2024-01-22T19:42:42.631713","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define transformer model inference method\n\ndef get_predictions_tranformer(test):\n    #model_checkpoint_infer = \"/kaggle/input/detect-llm-models/distilroberta-finetuned_v5/checkpoint-13542\"\n    \n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_infer, num_labels=2)\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    test_ds = Dataset.from_pandas(test)\n    test_ds_enc = test_ds.map(preprocess_function_infer, batched=True)\n    trainer = Trainer(model,tokenizer=tokenizer_infer,)\n    test_preds = trainer.predict(test_ds_enc)\n    logits = test_preds.predictions\n    final_preds_trans = sigmoid(logits)[:,0]\n    \n    return final_preds_trans\n","metadata":{"papermill":{"duration":0.014939,"end_time":"2024-01-22T19:42:42.669381","exception":false,"start_time":"2024-01-22T19:42:42.654442","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define transformer model training method\ndef train_inference_transformer_runtime(train,train_from_sub,test):\n    \n    #train = append_train_from_sub_phase(train, train_from_sub)\n    valid = train_from_sub\n    test = test[['id', 'text']]\n\n#     sk = StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n#     train0 = train\n#     for i, (tr,val) in enumerate(sk.split(train0,train0.label)):\n#         train = train0.iloc[tr]\n#         valid = train0.iloc[val]\n#         break\n        \n    train.text = train.text.fillna(\"\")\n    valid.text = valid.text.apply(lambda x: x.strip('\\n'))\n    train.text = train.text.apply(lambda x: x.strip('\\n'))\n    \n    ds_train = Dataset.from_pandas(train)\n    ds_valid = Dataset.from_pandas(valid)\n    \n    #tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_base)\n    ds_train_enc = ds_train.map(preprocess_function_train, batched=True)\n    ds_valid_enc = ds_valid.map(preprocess_function_train, batched=True)\n    \n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_base, num_labels=2)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    early_stopping = EarlyStoppingCallback(early_stopping_patience=5)\n    \n    num_train_epochs=10.0\n    metric_name = \"roc_auc\"\n    model_name = \"distilroberta\"\n    batch_size = 2\n    \n    args = TrainingArguments(\n        f\"{model_name}-finetuned_v5\",\n        evaluation_strategy = \"epoch\",\n        save_strategy = \"epoch\",\n        learning_rate=2e-5,\n        lr_scheduler_type = \"cosine\",\n        save_safetensors = False,\n        optim=\"adamw_torch\",\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        gradient_accumulation_steps=8,\n        num_train_epochs=num_train_epochs,\n        weight_decay=0.01,\n        load_best_model_at_end=True,\n        metric_for_best_model=metric_name,\n        save_total_limit=2,\n    )\n    \n    trainer = Trainer(\n        model,\n        args,\n        train_dataset=ds_train_enc,\n        eval_dataset=ds_valid_enc,\n        tokenizer=tokenizer_train,\n        callbacks = [early_stopping],\n        compute_metrics=compute_metrics\n    )\n    \n    trainer.train()\n    \n    trained_model = trainer.model\n    \n    test_ds = Dataset.from_pandas(test)\n    test_ds_enc = test_ds.map(preprocess_function_infer, batched=True)\n    trainer = Trainer(trained_model,tokenizer=tokenizer_train,)\n    test_preds = trainer.predict(test_ds_enc)\n    logits = test_preds.predictions\n    probs = sigmoid(logits)[:,1]\n    \n    return probs","metadata":{"papermill":{"duration":0.019765,"end_time":"2024-01-22T19:42:42.695653","exception":false,"start_time":"2024-01-22T19:42:42.675888","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#methods for selecting weak rows from test data \n\ndef get_selected_rows_from_test(test, X, final_predictions, final_preds_trans):\n    test = read_test()\n    test['label'] = final_predictions\n    test['trans_label'] = final_preds_trans\n    test = test[['id','text', 'label','trans_label']].copy()\n\n\n    # Calculate the median of the label values\n    median_label = test['label'].median()\n\n    # Compute the absolute difference from the median for each row\n    test['difference_from_median'] = abs(test['label'] - median_label)\n\n    # Sort the DataFrame by this difference\n    test_sorted = test.sort_values('difference_from_median')\n\n    # Select the top X rows (replace X with your desired number of rows)\n    #X = 50  # For example, to select 10 rows\n    selected_rows = test_sorted.head(X)\n    selected_rows = selected_rows.drop(columns=['difference_from_median'])\n\n    return selected_rows\n\ndef get_similar_train_text(selected_rows, number_of_similar_rows, train, lm_data, train_old):\n    #train, lm_data, train_old = read_train(only_7_prompts = True)\n\n    print(\"train tokenizer ... iter: similarity\")\n    vs_counters = train_tokenizer(train, lm_data, train_old, selected_rows)\n\n    print(\"tokenize datasets ... iter: similarity\")\n    tokenized_texts_train, tokenized_texts_test, tokenized_texts_test2 = tokenize_datasets (vs_counters, train, lm_data, selected_rows)\n\n    print(\"verctorize datasets ... similarity \")\n    tfidf_train, tfidf_selected = vectorizer_of_data(tokenized_texts_train,tokenized_texts_test,tokenized_texts_test2, 2, test )\n\n    del tokenized_texts_test2, tokenized_texts_test, tokenized_texts_train, vs_counters\n    gc.collect()\n\n    similar_texts_dfs = []\n\n    # Iterate over the TF-IDF vectors of selected_rows\n    for i in range(tfidf_selected.shape[0]):\n        # Calculate cosine similarity between the selected text and all train texts\n        cosine_similarities = cosine_similarity(tfidf_selected[i], tfidf_train)\n\n        # Sort the similarities and get the top ones (e.g., top 10)\n        top_indices = cosine_similarities.argsort()[0][-number_of_similar_rows:]  # Adjust number as needed\n\n        # Retrieve the similar texts from train_df\n        similar_texts = train.iloc[top_indices]\n\n        # Add the DataFrame of similar texts to the list\n        similar_texts_dfs.append(similar_texts)\n\n    # Concatenate all the DataFrames in the list to form the final DataFrame\n    similar_texts_df = pd.concat(similar_texts_dfs).drop_duplicates()\n\n    return similar_texts_df","metadata":{"papermill":{"duration":0.018736,"end_time":"2024-01-22T19:42:42.720907","exception":false,"start_time":"2024-01-22T19:42:42.702171","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define training X times strong procedures\n\ndef Train_Linear_X_Times_With_Infer_STRONG(number_of_times, X_TOP, Y_BOTTOM, test_data, only_7_prompts, trans_predictions, test_in,train_in, lm_data_in, train_old_in):\n    \n    final_preds_trans_DistilRoberta = trans_predictions\n    first_time = True\n    feedback_times = number_of_times\n    final_predictions = []\n    X = X_TOP\n    Y = Y_BOTTOM\n    \n    train_from_sub = read_test() #dummy def\n    \n    for i in range(1, feedback_times+1):\n        print(\"reading datasets ... iter: \", i)\n        #sub = read_sub()\n        #test = test_data  #read_dummy_test() #read_test()\n        #train, lm_data, train_old = read_train(only_7_prompts = only_7_prompts)\n        test = test_in.copy()\n        train = train_in.copy()\n        lm_data = lm_data_in.copy()\n        train_old = train_old_in.copy()\n    \n        if first_time == False:\n            train = append_train_from_sub_phase(train, train_from_sub)\n        \n        print(\"train tokenizer ... iter: \", i)\n        vs_counters = train_tokenizer(train, lm_data, train_old, test)\n    \n        print(\"tokenize datasets ... iter: \", i)\n        tokenized_texts_train, tokenized_texts_test, tokenized_texts_test2 = tokenize_datasets (vs_counters, train, lm_data, test)\n    \n        print(\"verctorize datasets ...iter: \", i)\n        tf_train, tf_test = vectorizer_of_data(tokenized_texts_train,tokenized_texts_test,tokenized_texts_test2, 2, test )\n        \n        del tokenized_texts_test2, tokenized_texts_test, tokenized_texts_train, vs_counters\n        gc.collect()\n    \n        print(\"predictions ... iter: \", i)\n        X_train_scaled, X_test_scaled = MaxAbsScalerTransform(tf_train,tf_test)\n        final_preds_linear_tmp = get_predictions_linear_LinearSVR(X_train_scaled, X_test_scaled, train['label'].values)\n    \n        del tf_train, tf_test, X_train_scaled, X_test_scaled \n        gc.collect()\n    \n        print(\"predeictions from iter : \",i, \" is : \", final_preds_linear_tmp)\n        \n        if first_time == True:\n            final_preds_phase_tmp = 0.5*final_preds_linear_tmp + 0.5*final_preds_trans_DistilRoberta\n        else:\n            final_preds_phase_tmp = 0.65*final_preds_linear_tmp + 0.35*final_preds_trans_DistilRoberta\n    \n        #final_preds_phase_tmp = 0.5*final_preds_linear_tmp + 0.5*final_preds_trans_DistilRoberta\n    \n        print(\"final predeictions from iter : \",i, \" is : \", final_preds_phase_tmp)\n\n        final_predictions = final_preds_phase_tmp #final_preds_phase_tmp\n    \n        train_from_sub = build_new_train_from_sub_all(final_preds_phase_tmp, test, X, Y)\n        X = X + int(250/i)\n        Y = Y + int(250/i)\n        print(\"new x,y: \",X,Y)\n    \n        first_time = False\n        \n    return final_predictions, train_from_sub\n    ","metadata":{"papermill":{"duration":0.020744,"end_time":"2024-01-22T19:42:42.748134","exception":false,"start_time":"2024-01-22T19:42:42.72739","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define training X times weak procedures\ndef Train_Linear_With_Infer_WEAK(X_MIDDLE,SIMILAR_ROWS, test_data, only_7_prompts, final_predictions, final_preds_trans, train, lm_data, train_old):\n    \n    test = test_data\n    selected_rows = get_selected_rows_from_test(test,X_MIDDLE,final_predictions, final_preds_trans)\n    similar_texts_df = get_similar_train_text(selected_rows, SIMILAR_ROWS,train, lm_data, train_old)\n    \n    #train, lm_data, train_old = read_train(only_7_prompts = only_7_prompts)\n    train = similar_texts_df\n    train = train.drop_duplicates(subset='text', keep='first')\n    train = train[['text', 'label']]\n    train.reset_index(drop=True, inplace=True)\n        \n    test = selected_rows \n        \n    print(\"train tokenizer ... iter: over weak\")\n    vs_counters = train_tokenizer(train, lm_data, train_old, test)\n        \n    print(\"tokenize datasets ... iter: over weak\")\n    tokenized_texts_train, tokenized_texts_test, tokenized_texts_test2 = tokenize_datasets (vs_counters, train, lm_data, test)\n        \n    print(\"verctorize datasets ...iter: over weak\")\n    tf_train, tf_test = vectorizer_of_data(tokenized_texts_train,tokenized_texts_test,tokenized_texts_test2, 2 ,test)\n        \n    del tokenized_texts_test2, tokenized_texts_test, tokenized_texts_train, vs_counters\n    gc.collect()\n        \n    print(\"predictions ... iter: over weak\")\n    X_train_scaled, X_test_scaled = MaxAbsScalerTransform(tf_train,tf_test)\n    preds_linear_weak= get_predictions_linear_LinearSVR(X_train_scaled, X_test_scaled, train['label'].values)\n    \n    del tf_train, tf_test, X_train_scaled, X_test_scaled \n    gc.collect()\n    \n    trans_sel_pred = selected_rows['trans_label']\n    trans_sel_pred = trans_sel_pred.values\n    selected_rows_final_pred = 0.5*preds_linear_weak +0.5*trans_sel_pred\n    selected_rows['label'] = selected_rows_final_pred\n    \n    test = read_test()\n    test['label'] = final_predictions\n    \n    test.set_index('id', inplace=True, drop=True)\n    selected_rows.set_index('id', inplace=True, drop=True)\n    # Update test_df with the new label values from selected_rows\n    test.update(selected_rows)\n    # Reset index if you want to revert 'id' back to a column\n    test.reset_index(inplace=True)\n\n    return test['label']\n","metadata":{"papermill":{"duration":0.019213,"end_time":"2024-01-22T19:42:42.774306","exception":false,"start_time":"2024-01-22T19:42:42.755093","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_for_prompt_names():\n    train_for_cat, lm_data, train_old = read_train_all()\n    tfidf_cat = TfidfVectorizer()\n    X_cat = tfidf_cat.fit_transform(train_for_cat['text'])\n    X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, train_for_cat['prompt_name'], test_size=0.05, random_state=42)\n    model_cat = LogisticRegression(max_iter=1000)\n    model_cat.fit(X_train_cat, y_train_cat)\n    return tfidf_cat, model_cat\n\n\ndef get_test_prompt_names(test,tfidf_cat,model_cat):\n    unique_prompt_ids_count = test['prompt_id'].nunique()\n    print(\"There are : \", unique_prompt_ids_count, \" prompts in the test data\")\n\n    predefined_prompt_names_list = ['Facial action coding system', 'Driverless cars', 'Exploring Venus', 'The Face on Mars', 'A Cowboy Who Rode the Waves']\n    if len(test) == 3:\n        print(\"select predefined prompts .. \")\n        return predefined_prompt_names_list\n\n    # predict test cats category \n    test_cats = tfidf_cat.transform(test['text'])\n    test_prompt_names = model_cat.predict(test_cats)\n    test['prompt_name'] = test_prompt_names\n    # get top n\n    prompt_counts = test['prompt_name'].value_counts()\n    top_n_prompts = prompt_counts.head(unique_prompt_ids_count).index.tolist()\n    return top_n_prompts\n","metadata":{"papermill":{"duration":0.016798,"end_time":"2024-01-22T19:42:42.797714","exception":false,"start_time":"2024-01-22T19:42:42.780916","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check if run is for save or submit\nsubmit_mode = True\nif len(pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')) == 3:\n    submit_mode = False","metadata":{"papermill":{"duration":0.023183,"end_time":"2024-01-22T19:42:42.827407","exception":false,"start_time":"2024-01-22T19:42:42.804224","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_mode = True\n","metadata":{"papermill":{"duration":0.012704,"end_time":"2024-01-22T19:42:42.846706","exception":false,"start_time":"2024-01-22T19:42:42.834002","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if submit_mode == True:\n    print(\"Predict the prompt names used in test ...\")\n    test = read_test()\n    tfidf_cat, model_cat = train_for_prompt_names()\n    top_n_prompts = get_test_prompt_names(test,tfidf_cat,model_cat)\n    print(top_n_prompts)\n    \n    print(\"done ...\")\n    print(\"select training data based on the prompt names .. \")\n    \n    train_all, lm_data, train_old_all = read_train_all()\n    train = train_all[train_all['prompt_name'].isin(top_n_prompts)]\n    train = pd.concat([train, train_old_all] , ignore_index=True)\n    #lm_data = lm_data_all[lm_data_all['prompt_name'].isin(top_n_prompts)]\n    train_old = train_old_all.copy()\n    \n    test = read_test()\n    #train, lm_data, train_old = read_train(only_7_prompts = True)\n    \n    print(\"spelling process...\")\n    train['text'] = train['text'].progress_apply(sentence_correcter)\n    lm_data['text'] = lm_data['text'].progress_apply(sentence_correcter)\n    train_old['text'] = train_old['text'].progress_apply(sentence_correcter)\n    test['text'] = test['text'].progress_apply(sentence_correcter)\n\n    print(\"predictions from transformer\")\n    final_preds_trans_DistilRoberta = get_predictions_tranformer(read_test()) # you may want to try with corrected test data \n    print (\"Predictions from Trans: \", final_preds_trans_DistilRoberta)\n    final_predictions_strong, train_from_sub = Train_Linear_X_Times_With_Infer_STRONG(4,1000,1500,test,True, final_preds_trans_DistilRoberta, test,train, lm_data, train_old)\n    \n    print (\"Predictions from Strong: \", final_predictions_strong)\n    final_predictions_strong_weak = Train_Linear_With_Infer_WEAK(50,100, test, True, final_predictions_strong, final_preds_trans_DistilRoberta, train, lm_data, train_old)\n    print (\"Predictions from Weak: \", final_predictions_strong_weak)\n    \n    #train, lm_data, train_old = read_train(only_7_prompts = True)  #this might affect the output if was droped\n    final_preds_trans_DistilRoberta_with_test_feedback = train_inference_transformer_runtime(train,train_from_sub,test)\n    print (\"Predictions from trans with feedback: \", final_preds_trans_DistilRoberta_with_test_feedback)\n    \n    final_predictions_deep = 0.8*final_predictions_strong_weak+0.2*final_preds_trans_DistilRoberta_with_test_feedback\n    \n    sub = read_sub() #read_dummy_test() #read_sub()\n\n    sub['generated'] = final_predictions_deep\n    sub.to_csv('submission.csv', index=False)\n    \nelse:\n    sub = read_sub()\n    sub['generated'] = 1\n    sub.to_csv('submission.csv', index=False)\n    \n    \n    \n    ","metadata":{"papermill":{"duration":5452.025742,"end_time":"2024-01-22T21:13:34.879284","exception":false,"start_time":"2024-01-22T19:42:42.853542","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}